{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker built-in BlazingText(Supervised) example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Data Preparation](#Development-Environment-and-Data-Preparation)\n",
    "    1. [Setup](#Setup)  \n",
    "    2. [Data Preparation](#Data-Preparation)\n",
    "    3. [Preprocessing](#Preprocessing)\n",
    "3. [Starting Sagemaker Training Job](#Starting-Sagemaker-Training-Job)  \n",
    "    1. [Create BlazingText Container](#Create-BlazingText-Container)  \n",
    "    2. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)   \n",
    "    3. [Training](#Training)\n",
    "4. [Inference](#Inference)\n",
    "    1. [(Optional) Batch Inference](#(Optional)-Batch-Inference)\n",
    "    1. [Hosting / Inference](#Hosting-/-Inference)\n",
    "5. [Stop / Close the Endpoint](#Stop-/-Close-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "ビルトインアルゴリズムの一つであるBlazingTextでは、教師なしアルゴリズムであるWord2Vecと教師ありの分類アルゴリズムを提供しています。\n",
    "\n",
    "ビルトインアルゴリズムを使用する場合、学習とデプロイに関連するコードのほとんどを開発者が意識する必要がなくなる点も利点となります。    \n",
    "\n",
    "このノートブックでは、Amazon の商品レビューに対する感情分析、つまり、そのレビューが Positive (Rating が 5 or 4) か、Negative (Rating が 1 or 2)なのかを判定します。これは、文書を Positive か Negative に分類する2クラスの分類問題なので、**BlazingText**による教師あり学習を適用することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Amazon の商品レビューデータセットは[Registry of Open Data on AWS](https://registry.opendata.aws/)で公開されており、 以下からダウンロード可能です。    \n",
    "このノートブックでは、日本語のデータセットをダウンロードします。\n",
    "\n",
    "- データセットの概要    \n",
    "https://registry.opendata.aws/amazon-reviews/\n",
    "\n",
    "- 日本語のデータセット(readme.htmlからたどることができます）    \n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
    "\n",
    "以下では、データをダウンロードして解凍 (unzip) します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "download_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\" \n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File {} already exists. Skipped download.\".format(file_name))\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    print(\"File downloaded: {}\".format(file_path))\n",
    "    \n",
    "if os.path.exists(tsv_file_path):\n",
    "    print(\"File {} already exists. Skipped unzip.\".format(tsv_file_name))\n",
    "else:\n",
    "    with gzip.open(file_path, mode='rb') as fin:\n",
    "        with open(tsv_file_path, 'wb') as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "            print(\"File uznipped: {}\".format(tsv_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "ダウンロードしたデータには学習に不要なデータや直接利用できないデータもあります。以下の前処理で利用できるようにします。\n",
    "\n",
    "1. ダウンロードしたデータには不要なデータも含まれているので削除します。\n",
    "2. 2クラス分類 (positive が 1, negative が 0)となるように評価データを加工し、レビューデータをMeCabを使ってスペース区切りのデータにします。\n",
    "3. 学習データ、バリデーションデータ、テストデータに分けて、学習用にS3にデータをアップロードします。\n",
    "\n",
    "### データの確認\n",
    "\n",
    "タブ区切りの tsv ファイルを読んで1行目を表示してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep ='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不要なデータの削除\n",
    "\n",
    "今回利用しないデータは以下の2つです。必要なデータだけ選んで保存します。\n",
    "\n",
    "- 評価データ `star_rating` と レビューのテキストデータ `review_body` 以外のデータ\n",
    "- 評価が 3 のデータ (positive でも negative でもないデータ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_neg = df.loc[:, [\"star_rating\", \"review_body\"]]\n",
    "df_pos_neg = df_pos_neg[df_pos_neg.star_rating != 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeCab のインストール\n",
    "\n",
    "BlazingText は、文章をそのまま学習・推論に利用することはできず、語ごとにスペースで区切って利用する必要があります。これはスペースで区切られている英語などでは問題ありませんが、スペースで区切られていない日本語では追加の処理が必要になります。\n",
    "\n",
    "ここでは、形態素とよばれる語の単位に分解（分かち書き）する形態素解析ツール MeCab を利用します。MeCab は pip でインストールして利用することができます。冒頭に`!`を入れることで、シェルコマンドを実行できます。`import MeCab` としても問題ないか確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install mecab-python3\n",
    "!{sys.executable} -m pip install unidic-lite\n",
    "\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの加工\n",
    "\n",
    "BlazingText では以下のようなデータが必要です。\n",
    "\n",
    "```\n",
    "__label__1  私 は これ が 好き　です 。\n",
    "__label__0  私 は これ が きらい　です 。\n",
    "```\n",
    "\n",
    "`__label__数字` は文書のラベルを表します。negative `__label__0`、positive なら `__label__1` とします。ラベル以降は、文書をスペース区切りにしたものですので、各文に対して MeCab による形態素解析を実行します。全文の処理に2, 3分必要になる場合があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "\n",
    "def func_to_row(x):\n",
    "    if x[\"star_rating\"] < 3:\n",
    "        label = '0'\n",
    "    else:\n",
    "        label = '1'\n",
    "    x[\"star_rating\"] = \"__label__\" + label\n",
    "    x[\"review_body\"] = mecab.parse(x[\"review_body\"].replace('<br />', '')).replace('\\n', '')\n",
    "    return x\n",
    "\n",
    "labeled_df = df_pos_neg.apply(lambda x: func_to_row(x), axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データの分割\n",
    "\n",
    "すべてのデータを学習データとすると、データを使って作成したモデルが良いのか悪いのか評価するデータが別途必要になります。\n",
    "そこで、データを学習データ、バリデーションデータ、テストデータに分割して利用します。学習データはモデルの学習に利用し、バリデーションデータは学習時のモデルの評価に利用します。最終的に作成されたモデルに対してテストデータによる評価を行います。\n",
    "\n",
    "`train_ratio` で設定した割合のデータを学習データとし、残ったデータをバリデーションとデータテストデータに分割して利用します。学習に利用する学習データとバリデーションデータは、後にSageMakerで利用するために、`savetxt` を利用してスペース区切りの csv に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_size = len(labeled_df.index)\n",
    "train_ratio = 0.8\n",
    "train_index = np.random.choice(data_size, int(data_size*train_ratio), replace=False)\n",
    "other_index = np.setdiff1d(np.arange(data_size), train_index)\n",
    "valid_index = np.random.choice(other_index, int(len(other_index)/2), replace=False)\n",
    "test_index = np.setdiff1d(np.arange(data_size), np.concatenate([train_index, valid_index]))\n",
    "\n",
    "np.savetxt('train.csv',labeled_df.iloc[train_index].values, fmt=\"%s %s\", delimiter=' ') \n",
    "np.savetxt('validation.csv',labeled_df.iloc[valid_index].values, fmt=\"%s %s\", delimiter=' ') \n",
    "\n",
    "print(\"Data is splitted into:\")\n",
    "print(\"Training data: {} records.\".format(len(train_index)))\n",
    "print(\"Validation data: {} records.\".format(len(valid_index)))\n",
    "print(\"Test data: {} records.\".format(len(test_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "SageMaker での学習に利用するために、学習データとバリデーションデータを S3 にアップロードします。SageMaker Python SDK の upload_data を利用すると、S3 にファイルをアップロードできます。アップロード先のバケットは `sagemaker-{リージョン名}-{アカウントID}`で、バケットがない場合は自動作成されます。もし存在するバケットにアップロードする場合は、このバケット名を引数で指定できます。\n",
    "\n",
    "アップロードが終われば、TrainingInput を利用して、アップロードしたファイルの content_type などを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "s3_train_data = sess.upload_data(path='train.csv', key_prefix='amazon-review-data')\n",
    "s3_validation_data = sess.upload_data(path='validation.csv', key_prefix='amazon-review-data')\n",
    "print(\"Training data is uploaded to {}\".format(s3_train_data))\n",
    "print(\"Validation data is uploaded to {}\".format(s3_validation_data))\n",
    "\n",
    "train_data = TrainingInput(s3_train_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix')\n",
    "validation_data = TrainingInput(s3_validation_data, distribution='FullyReplicated', content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Sagemaker Training Job\n",
    "\n",
    "BlazingText はビルトインアルゴリズムなので、アルゴリズムの実装は不要です。BlazingTextのコンテナイメージを呼び出して実行します。`get_image_uri` を利用すればコンテナイメージの URI を取得することができます。 取得した URI とそれを実行するインスタンスなどを指定して、Estimator を呼び出すことで学習の設定を行うことができます。\n",
    "\n",
    "ビルトインアルゴリズムでは、実行内容を設定するいくつかのハイパーパラメータを設定する必要があります。BlazingText では `mode` のハイパーパラメータが必須です。テキスト分類を行う場合は `mode=\"supervised\"` の指定が必要です。\n",
    "\n",
    "BlazingTextは、FastTextテキスト分類器を拡張し、カスタムCUDAカーネルを使用してGPUアクセラレーションを活用しています。このモデルは、マルチコアCPUやGPUを使って数分で10億語以上の単語を学習することができ、最先端の深層学習テキスト分類アルゴリズムと同等のパフォーマンスを実現しています。詳細については、[algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)または[the text classification notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_text_classification_dbpedia/blazingtext_text_classification_dbpedia.ipynb)をご参照ください。\n",
    "\n",
    "要約すると、BlazingTextでは以下のモードが、異なるタイプのインスタンスでサポートされています。\n",
    "\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|         -       \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|     - \t|        -  \t|        ✔       \t|  -   | |\n",
    "\n",
    "最後に S3 のデータを指定して fit を呼べば学習を始めることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BlazingText Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region_name)\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    input_mode= 'File',\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "bt_model.set_hyperparameters(\n",
    "    mode=\"supervised\",\n",
    "    epochs=10,\n",
    "    vector_dim=10,\n",
    "    early_stopping=True,\n",
    "    patience=4,\n",
    "    min_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Batch Inference\n",
    "バッチ変換処理を使用してファイルに対して一括で推論を実行します。    \n",
    "ここではすでにトークナイズ、単語の頻度表現へ変換済みの学習データを使用しますが、新規のデータへ適用する場合は別途実行する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ビルトインのBatch推論は1ファイル6MBまでの制限があるため、データを小さくします（実運用で大きなデータを扱う際はデータを分割する必要があります）\n",
    "test_data = labeled_df.iloc[test_index][:1000]\n",
    "print(test_data.shape)\n",
    "test_data.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データのフォーマットは学習時とは異なり、json形式 or jsonline形式となります\n",
    "\n",
    "`content-type: application/json`\n",
    "```json\n",
    "{\n",
    " \"instances\": [\"the movie was excellent\", \"i did not like the plot .\"]\n",
    "}\n",
    "```\n",
    "top-kを予測したい場合は以下のように変更します。\n",
    "```json\n",
    "{\n",
    " \"instances\": [\"the movie was excellent\", \"i did not like the plot .\"],\n",
    " \"configuration\": {\"k\": 2}\n",
    "}\n",
    "```\n",
    "\n",
    "`content-type: application/jsonlines`\n",
    "```jsonline\n",
    "{\"source\": \"source_0\"}\n",
    "{\"source\": \"source_1\"}\n",
    "```\n",
    "top-kを予測したい場合は以下のように変更します。\n",
    "```jsonline\n",
    "{\"source\": \"source_0\", \"k\": 2}\n",
    "{\"source\": \"source_1\", \"k\": 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# json形式\n",
    "import json\n",
    "\n",
    "d = list(test_data.review_body)\n",
    "with open('test.json', 'w') as f:\n",
    "    for line in d:\n",
    "        json.dump({\"source\": line}, f, ensure_ascii=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# jsonline形式\n",
    "import json\n",
    "\n",
    "d = {\"instances\": list(test_data.review_body)}\n",
    "with open('test.json', 'w') as f:\n",
    "    json.dump(d, f, ensure_ascii=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "s3_test_data = sess.upload_data(path='test.json', key_prefix='amazon-review-data')\n",
    "print(\"Test data is uploaded to {}\".format(s3_test_data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "bucket = sess.default_bucket()\n",
    "output_path = f's3://{bucket}/amazon-review-data/output/blazingtext_batch_transform'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transformer = bt_model.transformer(\n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge', \n",
    "    output_path = output_path,\n",
    "    strategy = \"MultiRecord\",\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data = s3_test_data, \n",
    "    data_type = \"S3Prefix\", \n",
    "    content_type = \"application/json\", \n",
    "    split_type = \"Line\",\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download-predict-file-from-s3\n",
    "バッチ推論されたアウトプットファイルをS3からダウンロードして、精度を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sagemaker.s3 import S3Downloader, s3_path_join\n",
    "\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = \"test.json.out\"\n",
    "output_path = s3_path_join(output_path, output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path, '.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(output_file) as f:\n",
    "    output = json.load(f)\n",
    "    print(output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "\n",
    "学習が終わると、作成されたモデルをデプロイして、推論を実行することができます。デプロイは deploy を呼び出すだけでできます。`---`といった出力があるときはデプロイ中で、`!`が出力されるとデプロイが完了です。\n",
    "\n",
    "エンドポイントは json 形式でリクエストを受け付けますので、serializer の content_type に `application/json` を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m5.xlarge')\n",
    "text_classifier.serializer = sagemaker.serializers.IdentitySerializer(content_type = 'application/json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デプロイが終わったら推論を実行してみましょう。ここでは negative なレビューを 5件、 positive なレビューを 5件ランダムに選択して推論を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "num_test = 5\n",
    "test_data = labeled_df.iloc[test_index]\n",
    "\n",
    "neg_test_data = test_data[test_data.star_rating == '__label__0']\n",
    "pos_test_data = test_data[test_data.star_rating == '__label__1']\n",
    "\n",
    "neg_index = np.random.choice(neg_test_data.index, num_test)\n",
    "pos_index = np.random.choice(pos_test_data.index, num_test)\n",
    "\n",
    "neg_test_sentences = [text for text in neg_test_data.loc[neg_index][\"review_body\"].values]\n",
    "payload = {\"instances\" : neg_test_sentences}\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "predictions = json.loads(response)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"Ground Truth: {}, Prediction: {} (probability: {})\"\n",
    "                      .format(0, pred[\"label\"][0][-1], pred[\"prob\"]))\n",
    "    print(neg_test_sentences[i].replace(' ', ''))\n",
    "    print()\n",
    "    \n",
    "pos_test_sentences = [text for text in pos_test_data.loc[pos_index][\"review_body\"].values]\n",
    "payload = {\"instances\" : pos_test_sentences}\n",
    "response = text_classifier.predict(json.dumps(payload))\n",
    "predictions = json.loads(response)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"Ground Truth: {}, Prediction: {} (probability: {})\"\n",
    "                      .format(1, pred[\"label\"][0][-1], pred[\"prob\"]))\n",
    "    print(pos_test_sentences[i].replace(' ', ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
