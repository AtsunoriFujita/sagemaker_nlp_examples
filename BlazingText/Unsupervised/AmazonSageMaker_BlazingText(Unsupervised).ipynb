{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker built-in BlazingText(Unsupervised) example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Data Preparation](#Development-Environment-and-Data-Preparation)\n",
    "    1. [Setup](#Setup)  \n",
    "    2. [Data Preparation](#Data-Preparation)    \n",
    "    3. [(Optinal)Use the latest wikipedia](#(Optinal)Use-the-latest-wikipedia)\n",
    "3. [Training the BlazingText model for generating word vectors](#Training-the-BlazingText-model-for-generating-word-vectors)  \n",
    "    1. [Create BlazingText Container](#Create-BlazingText-Container)  \n",
    "    2. [Set Hyperparameters](#Set-Hyperparameters)   \n",
    "    3. [Training](#Training)\n",
    "4. [Hosting / Inference](#Hosting-/-Inference)\n",
    "    1. [Use JSON format for inference](#Use-JSON-format-for-inference)\n",
    "    2. [Evaluation](#Evaluation)\n",
    "    3. [Model Artifacts for the Word2Vec Algorithm](#Model-Artifacts-for-the-Word2Vec-Algorithm)\n",
    "5. [Stop / Close the Endpoint](#Stop-/-Close-the-Endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Word2Vecは、教師なし学習を用いて、大規模なコーパス内の単語のベクトル表現を生成するためのポピュラーなアルゴリズムです。    \n",
    "生成されたベクトルは、対応する単語間の意味的な関係を捉えていることが示されており、感情分析、固有表現抽出、機械翻訳など、多くの自然言語処理（NLP）タスクで使用されています。    \n",
    "\n",
    "SageMaker BlazingTextは、以下の環境でのWord2Vecの効率的な実装を提供しています。\n",
    "\n",
    "- 単一のCPUインスタンス\n",
    "- 単一のGPUインスタンス上でのマルチGPU - P2 or P3インスタンス\n",
    "- 複数のCPUインスタンス(分散学習)\n",
    "\n",
    "ビルトインアルゴリズムを使用する場合、学習とデプロイに関連するコードのほとんどを開発者が意識する必要がなくなる点も利点となります。    \n",
    "このノートブックでは、BlazingTextで複数のCPUインスタンスを使ったword2vecの分散学習がどのように使用できるかを示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development Environment and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "- モデルデータの保存に使用するS3バケットとプレフィックス、およびトレーニングデータが置かれている場所はノートブックインスタンス、トレーニングインスタンス、およびホスティングインスタンスと同じリージョン内にある必要があります。バケットを指定しない場合、SageMaker SDKは同じリージョン内にあらかじめ定義された命名規則に従ってデフォルトのバケットを作成します。\n",
    "- IAM ロール ARN は、SageMaker にデータへのアクセスを与えるために使用されます。SageMaker python SDK の **get_execution_role** メソッドを使用して取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)  # This is the role that SageMaker would use to leverage AWS resources (S3, CloudWatch) on your behalf\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "output_bucket = sess.default_bucket()  # Replace with your own bucket name if needed\n",
    "print(output_bucket)\n",
    "output_prefix = \"sagemaker/DEMO-blazingtext-text8\"  # Replace with the prefix under which you want to store the data if needed\n",
    "\n",
    "data_bucket = f\"sagemaker-sample-files\"  # Replace with the bucket where your data is located\n",
    "data_prefix = \"datasets/text/text8/text8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlazingTextでは、スペースで区切られたトークンを含む、1行1文の前処理済みのテキストファイルを想定しています。    \n",
    "このノートブックでは、[日本語版text8](https://github.com/Hironsan/ja.text8)データセット(100MB)を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3-ap-northeast-1.amazonaws.com/dev.tech-sketch.jp/chakki/public/ja.text8.zip\n",
    "!unzip ja.text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optinal)Use the latest wikipedia\n",
    "\n",
    "https://dumps.wikimedia.org/jawiki を使用して最新のwikipedeaでデータセットを作成します。\n",
    "\n",
    "使用するインスタンスタイプにも依存しますがこの作業には約1h時間程度かかります。    \n",
    "[日本語版text8コーパスを作って分散表現を学習する](https://hironsan.hatenablog.com/entry/japanese-text8-corpus)を参考に作成しています。\n",
    "\n",
    "**_Note:_ 3GBを超えるデータをダウンロードします。ストレージの空き容量に注意してください**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dumps.wikimedia.org/jawiki/20210720/jawiki-20210720-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wikiextractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python -m wikiextractor.WikiExtractor -o extracted jawiki-20210720-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mecab-python3\n",
    "!pip install unidic\n",
    "!python -m unidic download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python process.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python tokenize_and_sampling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"ja.text8\")\n",
    "words = f.read().split()\n",
    "print('総単語数: ', len(words))\n",
    "print('異なり語数: ', len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Amazon S3 bucket\n",
    "\n",
    "用意したデータをS3にアップロードし、モデルのアーティファクトが保存されるS3の出力場所を設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(\"ja.text8\", output_bucket, output_prefix + \"/train\")\n",
    "\n",
    "s3_train_data = f\"s3://{output_bucket}/{output_prefix}/train\"\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_train_data)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the BlazingText model for generating word vectors\n",
    "\n",
    "[Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)のオリジナルの実装と同様に、SageMaker BlazingText はネガティブサンプリングを使用して、continuous bag-of-words (CBOW) および skip-gramの効率的な実装をCPUおよびGPU上で提供します。\n",
    "\n",
    "GPU実装では、高度に最適化されたCUDAカーネルを使用しています。詳細は、[*BlazingText: Scaling and Accelerating Word2Vec using Multiple GPU*](https://dl.acm.org/citation.cfm?doid=3146347.3146354)を参照してください。 \n",
    "\n",
    "BlazingTextは`CBOW`モードと`Skip-gram`モードでサブワード埋め込みの学習をサポートしています。これにより、BlazingTextは、この[notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_word2vec_subwords_text8/blazingtext_word2vec_subwords_text8.ipynb)で示されているように、Out-Of-Vocabulary(OOV)のない単語ベクトルを生成することができます。\n",
    "\n",
    "SageMaker BlazingTextでは、CBOWやSkip-gramの他に、効率的なミニバッチや行列-行列演算（[BLAS Level 3 routines](https://software.intel.com/en-us/mkl-developer-reference-fortran-blas-level-3-routines)）を使用した`Batch Skipgram`モードもサポートしています。このモードでは、複数のCPUノードに分散してword2vecの学習を行うことができ、1秒間に数億語を処理するword2vec計算をほぼリニアにスケールアップすることができます。詳細は[*Parallelizing Word2Vec in Shared and Distributed Memory*](https://arxiv.org/pdf/1604.04661.pdf)を参照してください。\n",
    "\n",
    "BlazingTextは、テキスト分類の**supervised**モードもサポートしています。BlazingTextは、FastTextテキスト分類器を拡張し、カスタムCUDAカーネルを使用してGPUアクセラレーションを活用しています。このモデルは、マルチコアCPUやGPUを使って数分で10億語以上の単語を学習することができ、最先端の深層学習テキスト分類アルゴリズムと同等のパフォーマンスを実現しています。詳細については、[algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html)または[the text classification notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/blazingtext_text_classification_dbpedia/blazingtext_text_classification_dbpedia.ipynb)をご参照ください。\n",
    "\n",
    "要約すると、BlazingTextでは以下のモードが、異なるタイプのインスタンスでサポートされています。\n",
    "\n",
    "\n",
    "|          Modes         \t| cbow (supports subwords training) \t| skipgram (supports subwords training) \t| batch_skipgram \t| supervised |\n",
    "|:----------------------:\t|:----:\t|:--------:\t|:--------------:\t| :--------------:\t|\n",
    "|   Single CPU instance  \t|   ✔  \t|     ✔    \t|        ✔       \t|  ✔  |\n",
    "|   Single GPU instance  \t|   ✔  \t|     ✔    \t|         -       \t|  ✔ (Instance with 1 GPU only)  |\n",
    "| Multiple CPU instances \t|     - \t|        -  \t|        ✔       \t|  -   | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、2台の`c5.2xlargeインスタンス`で`batch_skipgram`モードを使用して、*ja.text8*データセットで単語ベクトルを学習するためのリソース構成とハイパーパラメータを定義します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BlazingText Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region_name)\n",
    "print(f\"Using SageMaker BlazingText container: {container} ({region_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.c5.2xlarge\",\n",
    "    volume_size=5,\n",
    "    max_run=360000,\n",
    "    input_mode=\"File\",\n",
    "    output_path=s3_output_location,\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters\n",
    "\n",
    "ハイパーパラメータ については[algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(\n",
    "    mode=\"batch_skipgram\",\n",
    "    epochs=5,\n",
    "    min_count=5,\n",
    "    sampling_threshold=0.001,\n",
    "    learning_rate=0.05,\n",
    "    window_size=5,\n",
    "    vector_dim=100,\n",
    "    negative_samples=5,\n",
    "    batch_size=11,  #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "    evaluation=False,  # Perform similarity evaluation on WS-353 dataset at the end of training\n",
    "    subwords=False # Subword embedding learning is not supported by batch_skipgram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.inputs.TrainingInput(\n",
    "    s3_train_data,\n",
    "    distribution=\"FullyReplicated\",\n",
    "    content_type=\"text/plain\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    ")\n",
    "data_channels = {\"train\": train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "`Estimator`オブジェクトがあり、このオブジェクトのハイパーパラメータを設定し、データチャネルをアルゴリズムにリンクしています。あとは、アルゴリズムを学習するだけです。次のコマンドは、アルゴリズムを学習します。アルゴリズムの学習には、いくつかの手順が含まれます。\n",
    "\n",
    "1. `Estimator`クラスでリクエストしたインスタンスがプロビジョニングされ、適切なライブラリでセットアップされます。\n",
    "2. データがチャネルからインスタンスにダウンロードされます。\n",
    "3. トレーニングジョブが開始されます。\n",
    "\n",
    "データのサイズによっては、プロビジョニングとデータのダウンロードに時間がかかります。したがって、トレーニングジョブのトレーニングログの取得を開始するまでに数分かかる場合があります。\n",
    "\n",
    "ジョブが完了すると、「Job complete」メッセージが出力されます。トレーニングされたモデルは、Estimatorで `output_path`として設定されたS3バケットに保存されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Inference\n",
    "\n",
    "トレーニングジョブが完了すると、モデルをAmazonSageMakerリアルタイムホストエンドポイントとしてデプロイできます。    \n",
    "これにより、モデルから予測（または推論）を行うことができます。 トレーニングに使用したのと同じタイプのインスタンスでホストする必要はないことに注意してください。 インスタンスエンドポイントは長期間稼働するため、推論にはより安価なインスタンスを選択することをお勧めします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint = bt_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use JSON format for inference\n",
    "\n",
    "payloadには、キーが\"**instances**\"である単語のリストが含まれている必要があります。 BlazingTextはコンテンツタイプ `application / json`をサポートします。\n",
    "\n",
    "期待する挙動として、エンドポイントは各単語のn次元ベクトル（nはハイパーパラメーターで指定されたvector_dim）をレスポンスします。 単語がトレーニングデータセットにない場合、モデルはゼロベクトルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"日本\", \"五輪\"]\n",
    "payload = {\"instances\": words}\n",
    "\n",
    "response = bt_endpoint.predict(\n",
    "    json.dumps(payload),\n",
    "    initial_args={\"ContentType\": \"application/json\", \"Accept\": \"application/json\"},\n",
    ")\n",
    "\n",
    "vecs = json.loads(response)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "学習した単語ベクトルをダウンロードし、[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)を使用して視覚化してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "key = bt_model.model_data[bt_model.model_data.find(\"/\", 5) + 1 :]\n",
    "s3.Bucket(output_bucket).download_file(key, \"model.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model.tar.gz`を解凍して`vectors.txt`を使用します。\n",
    "\n",
    "BlazingTextでは、モデルアーティファクトは、単語からベクトルへのマッピングを含む`vectors.txt`とBlazingTextがホスティング、推論に使用するバイナリファイル`vectors.bin`で構成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Read the 400 most frequent word vectors. The vectors in the file are in descending order of frequency.\n",
    "num_points = 400\n",
    "\n",
    "first_line = True\n",
    "index_to_word = []\n",
    "\n",
    "with open(\"vectors.txt\", \"r\") as f:\n",
    "    for line_num, line in enumerate(f):\n",
    "        if first_line:\n",
    "            dim = int(line.strip().split()[1])\n",
    "            word_vecs = np.zeros((num_points, dim), dtype=float)\n",
    "            first_line = False\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        word = line.split()[0]\n",
    "        vec = word_vecs[line_num - 1]\n",
    "        for index, vec_val in enumerate(line.split()[1:]):\n",
    "            vec[index] = float(vec_val)\n",
    "        index_to_word.append(word)\n",
    "        if line_num >= num_points:\n",
    "            break\n",
    "word_vecs = normalize(word_vecs, copy=False, return_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=10000, random_state=42)\n",
    "two_d_embeddings = tsne.fit_transform(word_vecs[:num_points])\n",
    "labels = index_to_word[:num_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語対応\n",
    "!pip install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "import japanize_matplotlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "    pylab.figure(figsize=(20, 20))\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i, :]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(\n",
    "            label, xy=(x, y), xytext=(5, 2), textcoords=\"offset points\", ha=\"right\", va=\"bottom\"\n",
    "        )\n",
    "    pylab.show()\n",
    "\n",
    "\n",
    "plot(two_d_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Artifacts for the Word2Vec Algorithm\n",
    "\n",
    "`vector.txt`は、GensimやSpacyなどの他のツールと互換性のある形式でベクトルを保存します。 たとえば、Gensimユーザーは、次のコマンドを実行して、`vectors.txt`ファイルをロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(['日本'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['フランス', '東京'], negative=['パリ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.doesnt_match(\"東京 神奈川 千葉 埼玉 香港\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop / Close the Endpoint\n",
    "\n",
    "最後に、ノートブックを閉じる前にエンドポイントを削除する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(bt_endpoint.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
