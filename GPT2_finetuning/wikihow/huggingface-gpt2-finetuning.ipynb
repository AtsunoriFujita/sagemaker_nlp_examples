{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dceb97a",
   "metadata": {},
   "source": [
    "# Huggingface SageMaker-SDK - GPT2 Fine-tuning example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4794",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Permissions](#Permissions)\n",
    "    3. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "3. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    3. [Text Generation on Local](#Text-Generation-on-Local)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f2c987",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "このnotebookはHuggingFaceの[run_clm.py](https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py)を日本語データで動作する様に変更を加えたものです。    \n",
    "\n",
    "**日本語データで動作する様に変更を加えた以外はSageMakerで実行するために変更を加えた部分はありません**\n",
    "\n",
    "データは[wikiHow日本語要約データセット](https://github.com/Katsumata420/wikihow_japanese)を使用します。    \n",
    "\n",
    "このデモでは、AmazonSageMakerのHuggingFace Estimatorを使用してSageMakerのトレーニングジョブを実行します。    \n",
    "\n",
    "_**NOTE: このデモは、SagemakerNotebookインスタンスで動作検証しています**_    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae4501",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b13ebe",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "このNotebookはSageMakerの`conda_pytorch_p36`カーネルを利用しています。    \n",
    "\n",
    "**_Note: このnotebook上で推論テストを行う場合、（バージョンが古い場合は）pytorchのバージョンアップが必要になります。_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade torch\n",
    "!pip install \"sagemaker>=2.48.1\" \"transformers==4.9.2\" \"datasets[s3]==1.11.0\" --upgrade\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e71dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f851196",
   "metadata": {},
   "source": [
    "## Permissions\n",
    "\n",
    "ローカル環境でSagemakerを使用する場合はSagemakerに必要な権限を持つIAMロールにアクセスする必要があります。[こちら](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)を参照してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654bd90c",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "\n",
    "事前に`create_wikihow_dataset.ipynb`を実行してwikiHow日本語要約データセットを用意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac6b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec71530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('./wikihow_japanese/data/output/train.jsonl', orient='records', lines=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = pd.read_json('./wikihow_japanese/data/output/dev.jsonl', orient='records', lines=True)\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e41802",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.txt', 'w') as output_file:\n",
    "    for row in tqdm(train.itertuples(), total=train.shape[0]):\n",
    "        src = row.src\n",
    "        tgt = row.tgt\n",
    "\n",
    "        tokens = tokenizer.tokenize(src)\n",
    "        src = \"\".join(tokens).replace('▁', '')\n",
    "        text = '<s>' + src + '[SEP]' + tgt + '</s>'\n",
    "        output_file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dev.txt', 'w') as output_file:\n",
    "    for row in tqdm(dev.itertuples(), total=dev.shape[0]):\n",
    "        src = row.src\n",
    "        tgt = row.tgt\n",
    "\n",
    "        tokens = tokenizer.tokenize(src)\n",
    "        src = \"\".join(tokens).replace('▁', '')\n",
    "        text = '<s>' + src + '[SEP]' + tgt + '</s>'\n",
    "        output_file.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388354b",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "S3へデータをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03080395",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_prefix = 'samples/datasets/wikihow'\n",
    "\n",
    "input_train = sess.upload_data(\n",
    "    path='train.txt', \n",
    "    key_prefix=f'{s3_prefix}/train'\n",
    ")\n",
    "\n",
    "input_validation = sess.upload_data(\n",
    "    path='dev.txt', \n",
    "    key_prefix=f'{s3_prefix}/valid'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36fce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのUpload path\n",
    "\n",
    "print(input_train)\n",
    "print(input_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794b3163",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "`HuggingFace`のトレーニングジョブを作成するためには`HuggingFace` Estimatorが必要になります。    \n",
    "Estimatorは、エンドツーエンドのAmazonSageMakerトレーニングおよびデプロイタスクを処理します。 Estimatorで、どのFine-tuningスクリプトを`entry_point`として使用するか、どの`instance_type`を使用するか、どの`hyperparameters`を渡すかなどを定義します。\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    base_job_name='huggingface-sdk-extension',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    role=role,\n",
    "    hyperparameters={\n",
    "        'epochs': 1,\n",
    "        'train_batch_size': 32,\n",
    "        'model_name':'distilbert-base-uncased'\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "SageMakerトレーニングジョブを作成すると、SageMakerは`huggingface`コンテナを実行するために必要なec2インスタンスの起動と管理を行います。    \n",
    "Fine-tuningスクリプト`train.py`をアップロードし、`sagemaker_session_bucket`からコンテナ内の`/opt/ml/input/data`にデータをダウンロードして、トレーニングジョブを実行します。\n",
    "\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "`HuggingFace estimator`で定義した`hyperparameters`は、名前付き引数として渡されます。\n",
    "\n",
    "またSagemakerは、次のようなさまざまな環境変数を通じて、トレーニング環境に関する有用なプロパティを提供しています。\n",
    "\n",
    "* `SM_MODEL_DIR`：トレーニングジョブがモデルアーティファクトを書き込むパスを表す文字列。トレーニング後、このディレクトリのアーティファクトはモデルホスティングのためにS3にアップロードされます。\n",
    "\n",
    "* `SM_NUM_GPUS`：ホストで使用可能なGPUの数を表す整数。\n",
    "\n",
    "* `SM_CHANNEL_XXXX`：指定されたチャネルの入力データを含むディレクトリへのパスを表す文字列。たとえば、HuggingFace estimatorのfit呼び出しで`train`と`test`という名前の2つの入力チャネルを指定すると、環境変数`SM_CHANNEL_TRAIN`と`SM_CHANNEL_TEST`が設定されます。\n",
    "\n",
    "このトレーニングジョブをローカル環境で実行するには、`instance_type='local'`、GPUの場合は`instance_type='local_gpu'`で定義できます（GPUの場合は追加で設定が必要になります[SageMakerのドキュメント](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode)を参照してください）。    \n",
    "**_Note：これはSageMaker Studio内では機能しません_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements.txtはトレーニングジョブの実行前に実行されます（コンテナにライブラリを追加する際に使用します）\n",
    "# ファイルはここを参照しています。https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/requirements.txt\n",
    "# 1点異なる部分は transformers >= 4.8.0 でHuggingFaceコンテナのバージョンが古く本家に追いついていないため、バージョンアップを行なっています。\n",
    "!pygmentize ./scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf285e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングジョブで実行されるコード\n",
    "!pygmentize ./scripts/run_clm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d571c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_name_or_path':'rinna/japanese-gpt2-medium',\n",
    "    'train_file': '/opt/ml/input/data/train/train.txt',\n",
    "    'validation_file': '/opt/ml/input/data/validation/dev.txt',\n",
    "    'do_train': 'True',\n",
    "    'do_eval': 'True',\n",
    "    'num_train_epochs': 10,\n",
    "    'per_device_train_batch_size': 1,\n",
    "    'per_device_eval_batch_size': 1,\n",
    "    'use_fast_tokenizer': 'False',\n",
    "    'save_steps': 1000,\n",
    "    'save_total_limit': 1,\n",
    "    'output_dir':'/opt/ml/model',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22b0c93",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    entry_point='run_clm.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.8xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size=200,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84c4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': input_train, 'validation': input_validation})\n",
    "\n",
    "# ml.p3.8xlarge, 10 epochでの実行時間の目安\n",
    "# Training seconds: 3623\n",
    "# Billable seconds: 3623"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb684dd",
   "metadata": {},
   "source": [
    "## Download-fine-tuned-model-from-s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './output/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# 学習したモデルのダウンロード\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path='.', # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eab637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIRに解凍します\n",
    "\n",
    "!tar -zxvf model.tar.gz -C output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8821d",
   "metadata": {},
   "source": [
    "## Text Generation on Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fe7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, T5Tokenizer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained('output/')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c4dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(body, num_gen=5):\n",
    "    input_text = '<s>'+body+'[SEP]'\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    out = model.generate(input_ids, do_sample=True, top_p=0.95, top_k=40, \n",
    "                         num_return_sequences=num_gen, max_length=1024, bad_words_ids=[[1], [5]])\n",
    "    print('='*5,'原文', '='*5)\n",
    "    print(body)\n",
    "    print('-'*5, '要約', '-'*5)\n",
    "    for sent in tokenizer.batch_decode(out):\n",
    "        sent = sent.split('</s>')[1]\n",
    "        sent = sent.replace('</s>', '')\n",
    "        print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = dev.src[0]\n",
    "generate_summary(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = dev.src[1]\n",
    "generate_summary(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = dev.src[2]\n",
    "generate_summary(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e33b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
