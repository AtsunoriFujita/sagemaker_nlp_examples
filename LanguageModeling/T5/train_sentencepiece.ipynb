{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9fced1",
   "metadata": {},
   "source": [
    "# SentencePiece example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1bbd2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "このnotebookは言語モデル（ここではT5を想定）に使用するTokenizerを作成するサンプルノートブックです。    \n",
    "SagemakerNotebookインスタンスで動作検証していますがローカル含め他の環境でも動作すると思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea491f02",
   "metadata": {},
   "source": [
    "以下を参考にしています\n",
    "\n",
    "- https://github.com/google/sentencepiece\n",
    "- https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part7.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-datasets==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2436bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf74d78",
   "metadata": {},
   "source": [
    "以下のコマンドをターミナルから実行してSentencePieceをBuildしてください\n",
    "\n",
    "% cd sentencepiece    \n",
    "% mkdir build    \n",
    "% cd build    \n",
    "% cmake ..    \n",
    "% make -j $(nproc)    \n",
    "% sudo make install    \n",
    "% sudo ldconfig -v    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41870026",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './src/japanese-t5-base'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87202cd",
   "metadata": {},
   "source": [
    "ここでは'wikipedia/20190301.ja'を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98461ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "ds = tfds.load(\n",
    "    #name='wikipedia/20201201.ja', \n",
    "    name='wikipedia/20190301.ja', \n",
    "    shuffle_files=True,\n",
    "    download=True,\n",
    "    try_gcs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"].batch(128).prefetch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = []\n",
    "all_texts = []\n",
    "\n",
    "\n",
    "for example in tfds.as_numpy(train_ds):\n",
    "    titles, texts = example[\"title\"], example[\"text\"]\n",
    "    for title, text in zip(titles, texts):\n",
    "        all_titles.append(title.decode('utf-8'))\n",
    "        all_texts.append(text.decode('utf-8'))\n",
    "\n",
    "        \n",
    "with open(\"input.txt\", \"w\") as f:\n",
    "    for text in all_texts:\n",
    "        lines = [line.strip() for line in text.split(\"\\n\")]\n",
    "        for line in lines:\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            f.write(line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b8ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds, train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5493d45",
   "metadata": {},
   "source": [
    "入力データを一行一文形式へ加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c008d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat << EOF > preprocess.sh\n",
    "#!/bin/bash\n",
    "FILE=\\$1\n",
    "if [ \\$# -ne 1 ]; then\n",
    "  echo \"Usage: ./preprocess.sh INPUT_TEXT\"\n",
    "  exit 1\n",
    "fi\n",
    "echo \"Processing \\${FILE}\"\n",
    "sed -i -e '/^$/d; /<doc id/,+1d; s/<\\/doc>//g' \\${FILE}\n",
    "sed -i -e 's/ *$//g; s/。\\([^」|)|）|\"]\\)/。\\n\\1/g; s/^[ ]*//g' \\${FILE}\n",
    "sed -i -e '/^。/d' \\${FILE}\n",
    "sed -i -e 's/\\(.*\\)/\\L\\1/' \\${FILE}\n",
    "EOF\n",
    "chmod 744 preprocess.sh\n",
    "./preprocess.sh input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2493a9c",
   "metadata": {},
   "source": [
    "言語モデル（T5）が`build_sentencepiece_model.sh`のflagsを使って、ID=0がpadding、ID=1がEOS、ID=2がUNKになるように予約して構築されていることを前提にしているので、そのようにする\n",
    "\n",
    "**この処理はメモリを消費します。エラーが出る際はマシンのメモリを大きくする or 以下をコマンドに追加してサンプリングしてください**\n",
    "\n",
    "--input_sentence_size=12000000 \n",
    "--shuffle_input_sentence=true\n",
    "\n",
    "ちなみに--model_prefix=spmだとHuggingFaceでTokenizerをロードする際にエラーとなるので注意してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab19cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/local/bin/spm_train --input=./input.txt --model_prefix=\"spiece\" --vocab_size=32000 --character_coverage=0.9995 --model_type=unigram --pad_id=0 --eos_id=1 --unk_id=2 --bos_id=-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv spiece.model spiece.vocab ./src/japanese-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110bbed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
