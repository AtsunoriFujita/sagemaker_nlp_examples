{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6339c5a1",
   "metadata": {},
   "source": [
    "# Huggingface SageMaker-SDK - T5 Language Modeling example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54ccf9",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "このnotebookはSageMakerのHuggingFaceコンテナを使用して、言語モデル（ここではT5を想定）の学習を行います。   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3354ff0",
   "metadata": {},
   "source": [
    "以下を参考にしています\n",
    "- https://github.com/huggingface/transformers/tree/master/examples/flax/language-modeling\n",
    "\n",
    "- https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part7.html\n",
    "\n",
    "- https://github.com/megagonlabs/t5-japanese\n",
    "\n",
    "- https://arxiv.org/abs/1910.10683\n",
    "\n",
    "- https://github.com/google-research/text-to-text-transfer-transformer#gpu-usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a983eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-datasets==4.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfe002",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax>=0.2.8\n",
    "!pip install jaxlib>=0.1.59\n",
    "!pip install flax>=0.3.5\n",
    "!pip install optax>=0.0.9\n",
    "!pip install torch==1.10.0+cpu torchvision==0.11.1+cpu torchaudio==0.10.0+cpu -f https://download.pytorch.org/whl/cpu/torch_stable.html\n",
    "!pip install sentencepiece==0.1.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44856264",
   "metadata": {},
   "source": [
    "configファイルを言語モデル用のディレクトリに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config\n",
    "\n",
    "config = T5Config.from_pretrained(\"google/t5-v1_1-base\", vocab_size=32000)\n",
    "config.save_pretrained(\"./src/japanese-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5108a5fc",
   "metadata": {},
   "source": [
    "前工程で作成したTokenizerが動作するか確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccb71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"./src/japanese-t5-base\")\n",
    "tokenizer.tokenize(\"私は元気です。あなたは元気ですか？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d0d0eb",
   "metadata": {},
   "source": [
    "このサンプルノートブックではwiki40bを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b835dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://note.com/npaka/n/n0a2d0a4b806e\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "ds = tfds.load('wiki40b/ja', split='train', try_gcs=True)\n",
    "\n",
    "# データセットをテキスト形式で出力する関数\n",
    "def create_txt(file_name, tf_data):\n",
    "    start_paragraph = False\n",
    "\n",
    "    # ファイルの書き込み\n",
    "    with open(file_name, 'w') as f:\n",
    "         for wiki in tf_data.as_numpy_iterator():\n",
    "                for text in wiki['text'].decode().split('\\n'):\n",
    "                    if start_paragraph:\n",
    "                        text = text.replace('_NEWLINE_', '') # _NEWLINE_は削除\n",
    "                        f.write(text + '\\n')\n",
    "                        start_paragraph = False\n",
    "                    if text == '_START_PARAGRAPH_': # _START_PARAGRAPH_のみ取得\n",
    "                        start_paragraph = True\n",
    "\n",
    "# データセットをテキスト形式で出力\n",
    "create_txt('wiki_40b.txt', ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8260b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files='wiki_40b.txt', cache_dir=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8e9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95deec",
   "metadata": {},
   "source": [
    "データをS3へアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3f1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].to_json('train.json', force_ascii=False)\n",
    "\n",
    "s3_prefix = 'samples/datasets/wiki40b-jp'\n",
    "\n",
    "input_train = sess.upload_data(\n",
    "    path='train.json', \n",
    "    key_prefix=f'{s3_prefix}/train'\n",
    ")\n",
    "print(input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ce982",
   "metadata": {},
   "source": [
    "## Starting Sagemaker Training Job\n",
    "\n",
    "`HuggingFace`のトレーニングジョブを作成するためにはHuggingFace Estimatorが必要になります。\n",
    "Estimatorは、エンドツーエンドのAmazonSageMakerトレーニングおよびデプロイタスクを処理します。 Estimatorで、どのスクリプトをentry_pointとして使用するか、どのinstance_typeを使用するか、どのhyperparametersを渡すかなどを定義します。\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_t5_mlm_flax.py',\n",
    "    source_dir='./src',\n",
    "    instance_type='ml.p4d.24xlarge',\n",
    "    instance_count=1,\n",
    "    transformers_version='4.11',\n",
    "    #pytorch_version='1.9',\n",
    "    tensorflow_version='2.5',\n",
    "    py_version='py37',\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "```\n",
    "\n",
    "SageMakerトレーニングジョブを作成すると、SageMakerはhuggingfaceコンテナを実行するために必要なec2インスタンスの起動と管理を行います。\n",
    "`run_t5_mlm_flax.py`をアップロードし、sagemaker_session_bucketからコンテナ内の/opt/ml/input/dataにデータをダウンロードして、トレーニングジョブを実行します。\n",
    "\n",
    "HuggingFace estimatorで定義したhyperparametersは、名前付き引数として渡されます。\n",
    "\n",
    "またSagemakerは、次のようなさまざまな環境変数を通じて、トレーニング環境に関する有用なプロパティを提供しています。\n",
    "\n",
    "- `SM_MODEL_DIR`：トレーニングジョブがモデルアーティファクトを書き込むパスを表す文字列。トレーニング後、このディレクトリのアーティファクトはモデルホスティングのためにS3にアップロードされます。\n",
    "- `SM_NUM_GPUS`：ホストで使用可能なGPUの数を表す整数。\n",
    "- `SM_CHANNEL_XXXX`：指定されたチャネルの入力データを含むディレクトリへのパスを表す文字列。たとえば、HuggingFace estimatorのfit呼び出しでtrainとtestという名前の2つの入力チャネルを指定すると、環境変数SM_CHANNEL_TRAINとSM_CHANNEL_TESTが設定されます。\n",
    "\n",
    "このトレーニングジョブをローカル環境で実行するには、`instance_type='local'`、GPUの場合は`instance_type='local_gpu'`で定義できます（GPUの場合は追加で設定が必要になります[SageMakerのドキュメント](https://sagemaker.readthedocs.io/en/stable/overview.html#local-mode)を参照してください）。\n",
    "\n",
    "**Note：これはSageMaker Studio内では機能しません**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a8f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_type':'t5',\n",
    "    'train_file': '/opt/ml/input/data/train/train.json',\n",
    "    #'validation_file': '/opt/ml/input/data/validation/dev.csv',\n",
    "    #'test_file': '/opt/ml/input/data/test/test.csv',\n",
    "    'config_name':'./japanese-t5-base',\n",
    "    'tokenizer_name': './japanese-t5-base',\n",
    "    'max_seq_length': 512,\n",
    "    'per_device_train_batch_size': 32,\n",
    "    'per_device_eval_batch_size': 32,\n",
    "    'adafactor': 'True',\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 0.001,\n",
    "    'warmup_steps': 100,\n",
    "    'overwrite_output_dir': 'True',\n",
    "    'preprocessing_num_workers': 96,\n",
    "    'num_train_epochs': 1, \n",
    "    #'logging_strategy':  'epoch',\n",
    "    #'save_strategy': 'epoch',\n",
    "    #'evaluation_strategy': 'epoch',\n",
    "    'logging_steps': 200,\n",
    "    'save_steps': 500,\n",
    "    'eval_steps': 500,\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'push_to_hub':'False'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fde88",
   "metadata": {},
   "source": [
    "ここではサンプルのため、学習は1 epochのみとします。ml.p4d.24xlargeで30min程度で完了します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a9e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc3a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    role=role,\n",
    "    entry_point='run_t5_mlm_flax.py',\n",
    "    source_dir='./src',\n",
    "    instance_type='ml.p4d.24xlarge',\n",
    "    instance_count=1,\n",
    "    max_run=60*60*24*5,\n",
    "    volume_size=500,\n",
    "    transformers_version='4.11',\n",
    "    #pytorch_version='1.9',\n",
    "    tensorflow_version='2.5',\n",
    "    py_version='py37',\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffdb768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': input_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7548ed",
   "metadata": {},
   "source": [
    "## Download-model-from-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59838f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './output/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ef217",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# 学習したモデルのダウンロード\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path='.', # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f270b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIRに解凍します\n",
    "\n",
    "!tar -zxvf model.tar.gz -C output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2660d81",
   "metadata": {},
   "source": [
    "## (Optional) Convert from Flax Model to PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b2879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, FlaxT5ForConditionalGeneration\n",
    "\n",
    "mdl_path = \"./output\"\n",
    "\n",
    "pt_model = T5ForConditionalGeneration.from_pretrained(mdl_path, from_flax=True)\n",
    "pt_model.save_pretrained(mdl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7db088",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(mdl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fea9d02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
