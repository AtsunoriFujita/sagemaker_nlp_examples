{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker built-in Latent Dirichlet Allocation(LDA) example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Data Preparation](#Development-Environment-and-Data-Preparation)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Data Preparation](#Data-Preparation)    \n",
    "3. [Text Preprocessing](#Text-Preprocessing)  \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Create Corpus and Vocabulary](#Create-Corpus-and-Vocabulary)   \n",
    "4. [Training the LDA Model](#Training-the-LDA-Model)  \n",
    "    1. [Create LDA Container](#Create-LDA-Container)  \n",
    "    2. [Set Hyperparameters¶](#Set-Hyperparameters¶)   \n",
    "    3. [Training](#Training)\n",
    "5. [Evaluating the Model Output](#Evaluating-the-Model-Output)  \n",
    "    1. [Interpretation](#Interpretation)  \n",
    "    2. [Deploy and Inference](#Deploy-and-Inference)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "このnotebookはSageMakerのビルトインアルゴリズムの一つであるLatent Dirichlet Allocation(LDA)のサンプルです。    \n",
    "ビルトインアルゴリズムを使用する場合、学習とデプロイに関連するコードのほとんどを開発者が意識する必要がなくなる利点があります。    \n",
    "\n",
    "データはlivedoor ニュースコーパスを使用します。\n",
    "\n",
    "NOTE: このデモは、SagemakerNotebookインスタンスで動作検証しています"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Data Preparation\n",
    "\n",
    "## Installation\n",
    "このNotebookはSageMakerのconda_mxnet_p36カーネルを利用しています。    \n",
    "日本語処理のため、[GiNZA](https://megagonlabs.github.io/ginza/)などをインストールします。    \n",
    "\n",
    "_**NOTE: 日本語処理はmecabを使用するなど開発者の好みに変更することができます**_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install ginza==4.0.6\n",
    "!pip install mojimoji neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import tarfile\n",
    "import json\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "# Amazon Web Services (AWS) SDK for Python\n",
    "import boto3\n",
    "\n",
    "# SageMaker Python SDK\n",
    "import sagemaker\n",
    "from sagemaker.amazon.common import RecordSerializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "株式会社ロンウイットさんで公開している[livedoor ニュースコーパス](https://www.rondhuit.com/download.html)をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum update ca-certificates -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radiology-nlp.hatenablog.com/entry/2019/11/25/124219\n",
    "\n",
    "!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
    "!tar zxvf ldcc-20140209.tar.gz\n",
    "\n",
    "!echo -e \"filename\\tarticle\"$(for category in $(basename -a `find ./text -type d` | grep -v text | sort); do echo -n \"\\t\"; echo -n $category; done) > ./text/livedoor.tsv\n",
    "\n",
    "!for filename in `basename -a ./text/dokujo-tsushin/dokujo-tsushin-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/dokujo-tsushin/$filename`; echo -e \"\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/it-life-hack/it-life-hack-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/it-life-hack/$filename`; echo -e \"\\t0\\t1\\t0\\t0\\t0\\t0\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/kaden-channel/kaden-channel-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/kaden-channel/$filename`; echo -e \"\\t0\\t0\\t1\\t0\\t0\\t0\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/livedoor-homme/livedoor-homme-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/livedoor-homme/$filename`; echo -e \"\\t0\\t0\\t0\\t1\\t0\\t0\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/movie-enter/movie-enter-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/movie-enter/$filename`; echo -e \"\\t0\\t0\\t0\\t0\\t1\\t0\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/peachy/peachy-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/peachy/$filename`; echo -e \"\\t0\\t0\\t0\\t0\\t0\\t1\\t0\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/smax/smax-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/smax/$filename`; echo -e \"\\t0\\t0\\t0\\t0\\t0\\t0\\t1\\t0\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/sports-watch/sports-watch-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/sports-watch/$filename`; echo -e \"\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t1\\t0\"; done >> ./text/livedoor.tsv\n",
    "!for filename in `basename -a ./text/topic-news/topic-news-*`; do echo -n \"$filename\"; echo -ne \"\\t\"; echo -n `sed -e '1,3d' ./text/topic-news/$filename`; echo -e \"\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t0\\t1\"; done >> ./text/livedoor.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./text/livedoor.tsv', sep='\\t')\n",
    "df = df.sample(df.shape[0], random_state=42).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "ここではテキストを意味のある単位で分割していきたいのですが、日本語は英語とは異なり、各単語があらかじめスペースで区切られていません。    \n",
    "次のセルでは日本語NLPライブラリのGiNZAを使って文章を分割していきます。    \n",
    "\n",
    "また以下の処理     \n",
    "\n",
    "- URLの除去\n",
    "- htmlタグを除去\n",
    "- 文字の正規化、全角を半角に統一\n",
    "\n",
    "を行って、このサンプルでは名詞と形容詞のみを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import mojimoji\n",
    "import neologdn\n",
    "\n",
    "\n",
    "nlp = spacy.load('ja_ginza', disable=['ner'])\n",
    "stop_words = spacy.lang.ja.stop_words.STOP_WORDS\n",
    "\n",
    "\n",
    "def filterHtmlTag(txt):\n",
    "    soup = BeautifulSoup(txt, 'html.parser')\n",
    "    txt = soup.get_text(strip=True)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    result = mojimoji.zen_to_han(text, kana=False)\n",
    "    result = neologdn.normalize(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def text_to_words(text):\n",
    "    \n",
    "    basic_words = []\n",
    "    text = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-]+', '', text) # URLの除去\n",
    "    text = filterHtmlTag(text) # htmlタグの除去\n",
    "    text = normalize_text(text) # 正規化\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            if token.lemma_ in stop_words:\n",
    "                continue\n",
    "            \n",
    "            # 形容詞の原型を取得\n",
    "            elif token.pos_ in ('ADJ'):\n",
    "                basic_words.append(token.lemma_)\n",
    "                \n",
    "            # 名詞を取得\n",
    "            elif token.pos_ in ('NOUN'):\n",
    "                basic_words.append(token.orth_)\n",
    "        \n",
    "    basic_words = ' '.join(basic_words)\n",
    "    return basic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_words(df.article[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# m5.xlargeで約7minかかります\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool() as p:\n",
    "    docs = p.map(func=text_to_words, iterable=df.article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus and Vocabulary\n",
    "\n",
    "LDAの学習のためにコーパスと辞書を作成します。    \n",
    "- コーパスは各記事を単語の頻度表現（Bag of Words）にしたものです。    \n",
    "- 辞書は重複のない単語のリストです。    \n",
    "\n",
    "どちらもScikit-learnの`CountVectorizer`を使用して作成します。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizerの設定\n",
    "NGRAM=1\n",
    "MAX_DF=0.95\n",
    "MIN_DF=0.01\n",
    "NUM_VOCAB=None\n",
    "\n",
    "count_vec = CountVectorizer(ngram_range=(1, NGRAM), max_df=MAX_DF, min_df=MIN_DF, max_features=NUM_VOCAB)\n",
    "count_vec = count_vec.fit(docs)\n",
    "bags_of_words = count_vec.transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"count_vec.pkl\", \"wb\") as f:\n",
    "    pickle.dump(count_vec, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of bags_of_words : %s\" % (bags_of_words.shape,))\n",
    "\n",
    "vocab  = count_vec.get_feature_names()\n",
    "print(\"Num of vocab : %s\" % (len(vocab)))\n",
    "print(\"Sample of vocab : %s\" % (vocab[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './review_vocab.dat'\n",
    "\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to Amazon S3 bucket\n",
    "\n",
    "データを学習用、テスト用（推論用）に分割して、s3へアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbags_of_words = min(bags_of_words.shape[0], 10_000) # speed up testing with fewer documents\n",
    "\n",
    "nbags_of_words_training = int(0.95*nbags_of_words)\n",
    "nbags_of_words_test = nbags_of_words - nbags_of_words_training\n",
    "\n",
    "bags_of_words_training = bags_of_words[:nbags_of_words_training]\n",
    "bags_of_words_test = bags_of_words[nbags_of_words_training:nbags_of_words]\n",
    "\n",
    "print('training set dimensions = {}'.format(bags_of_words_training.shape))\n",
    "print('test set dimensions = {}'.format(bags_of_words_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、データをMXNet RecordIO Protobuf形式に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# convert documents_training to Protobuf RecordIO format\n",
    "recordio_protobuf_serializer = RecordSerializer()\n",
    "fbuffer = recordio_protobuf_serializer.serialize(bags_of_words_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = \"sagemaker/DEMO-lda-introduction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to S3 in bucket/prefix/train\n",
    "fname = 'lda.data'\n",
    "s3_object = os.path.join(prefix, 'train', fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(fbuffer)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, s3_object)\n",
    "print('Uploaded data to S3: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LDA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker LDAは、観測値の集合を異なるトピックの混合物として記述しようとする教師なし学習アルゴリズムです。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)\n",
    "\n",
    "- $M$: 文書数\n",
    "- $N$: 単語数\n",
    "- $\\alpha$: 文書ごとのトピック分布に対するディリクレ分布のパラメータ\n",
    "- $\\beta$: トピックごとの単語分布に対するディリクレ分布のパラメータ\n",
    "- $\\theta_m$: 文書mのトピック分布\n",
    "- $\\varphi_k$: トピックkの単語分布\n",
    "- $z_{mn}$: 文書mのn番目の単語の潜在トピック\n",
    "- $w_{mn}$: 文書mのn番目の単語(観測データ)\n",
    "\n",
    "パラメータ推定にはtensor spectral decompositionを使用しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LDA Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker LDA Docker container\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"lda\", region_name)\n",
    "\n",
    "print('Using SageMaker LDA container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters\n",
    "\n",
    "SageMaker LDAには以下のハイパーパラメータがあります。\n",
    "\n",
    "* **`num_topics`** - LDAモデル内のトピックまたはカテゴリの数\n",
    "    * 通常、これは事前にはわかりません\n",
    "\n",
    "* **`feature_dim`** - vocabularyのサイズ\n",
    "\n",
    "* **`mini_batch_size`** - 入力される文書の数\n",
    "\n",
    "* **`alpha0`** - *(optional)* トピック混合物の「混合度」\n",
    "  * `alpha0` が小さい場合、文書は1つまたは少数のトピックで表される傾向があります\n",
    "  * `alpha0` が大きい場合(１より大きい)、文書は複数または多数のトピックの均等な組み合わせになる傾向があります。\n",
    "  * デフォルト: `alpha0 = 1.0`.\n",
    "  \n",
    " \n",
    "SageMaker LDAは現在、シングルインスタンスのCPUトレーニングのみをサポートしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntopics = 20\n",
    "vocabulary_size = len(vocab)\n",
    "\n",
    "# specify general training job information\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role,\n",
    "    output_path = 's3://{}/{}/output'.format(bucket, prefix),\n",
    "    instance_count = 1,\n",
    "    instance_type = 'ml.c5.2xlarge',\n",
    "    sagemaker_session = session,\n",
    ")\n",
    "\n",
    "# set algorithm-specific hyperparameters\n",
    "lda.set_hyperparameters(\n",
    "    num_topics=ntopics,\n",
    "    feature_dim=vocabulary_size,\n",
    "    mini_batch_size=nbags_of_words_training,\n",
    "    alpha0=1.0,\n",
    "    max_restarts=10,\n",
    "    max_iterations=1000,\n",
    "    tol=1e-8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "上記の設定でアルゴリズムの実行時間は4-5分です"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training job on input data stored in S3\n",
    "start = time.time()\n",
    "try:\n",
    "    lda.fit({'train': s3_train_data})\n",
    "except RuntimeError as e:\n",
    "    print(e)  \n",
    "\n",
    "end = time.time()\n",
    "print(\"Training took\", end - start, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(lda.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model Output\n",
    "\n",
    "S3からモデルファイルをダウンロードし、検証します。    \n",
    "モデルは学習時に推定された$\\alpha$と$\\beta$のパラメータを含む2つの配列で構成されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract the model file from S3\n",
    "job_name = lda.latest_training_job.job_name\n",
    "model_fname = 'model.tar.gz'\n",
    "model_object = os.path.join(prefix, 'output', job_name, 'output', model_fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(model_object).download_file(fname)\n",
    "\n",
    "with tarfile.open(fname) as tar:\n",
    "    tar.extractall()\n",
    "print('Downloaded and extracted model tarball: {}'.format(model_object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the model file\n",
    "model_list = [fname for fname in os.listdir('.') if fname.startswith('model_')]\n",
    "model_fname = model_list[0]\n",
    "print('Found model file: {}'.format(model_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model from the model file and store in Numpy arrays\n",
    "alpha, beta = mx.ndarray.load(model_fname)\n",
    "learned_alpha = alpha.asnumpy()\n",
    "learned_beta = beta.asnumpy()\n",
    "\n",
    "print('\\nLearned alpha.shape = {}'.format(learned_alpha.shape))\n",
    "print('Learned beta.shape = {}'.format(learned_beta.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize alpha\n",
    "sns.lineplot(x=range(len(learned_alpha)), y=learned_alpha);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize beta\n",
    "sns.heatmap(learned_beta, vmax=0.01); # (topics, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_nr in range(ntopics):\n",
    "    # print most important words for a given topic\n",
    "\n",
    "    beta = learned_beta[topic_nr]\n",
    "    idx = np.argsort(beta)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Topic\", topic_nr)\n",
    "    print(\"=====================\")\n",
    "    for i in idx[:-16:-1]:\n",
    "        print(\"{:12} {:f}\".format(vocab[i], beta[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "トピックの数を20にして各トピックの重要単語を15個出力すると以下のような結果になりました。\n",
    "\n",
    "```\n",
    "Topic 0\n",
    "=====================\n",
    "チョコレート       0.014655\n",
    "人気           0.012782\n",
    "女性           0.012509\n",
    "クリスマス        0.012506\n",
    "限定           0.011682\n",
    "ブランド         0.010410\n",
    "サイト          0.010372\n",
    "多い           0.009846\n",
    "女子           0.009572\n",
    "アイテム         0.009305\n",
    "話題           0.009260\n",
    "cm           0.008218\n",
    "自分           0.008210\n",
    "商品           0.008137\n",
    "関連           0.007626\n",
    "\n",
    "Topic 1\n",
    "=====================\n",
    "映画           0.076599\n",
    "作品           0.027791\n",
    "公開           0.024247\n",
    "監督           0.020890\n",
    "本作           0.019164\n",
    "世界           0.016903\n",
    "映像           0.015342\n",
    "今回           0.010171\n",
    "全国           0.009590\n",
    "サイト          0.009230\n",
    "movie        0.008435\n",
    "記事           0.008399\n",
    "主演           0.008226\n",
    "公式           0.008088\n",
    "特集           0.007954\n",
    "\n",
    "Topic 2\n",
    "=====================\n",
    "....\n",
    "```\n",
    "\n",
    "Topic0の解釈はいろいろと考えられますが、Topic1は直感的には映画を指しているように思われます。    \n",
    "num_topicsを変更していろいろ試してみましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Batch Inference\n",
    "バッチ変換処理を使用してファイルに対して一括で推論を実行します。    \n",
    "ここではすでにトークナイズ、単語の頻度表現へ変換済みの学習データを使用しますが、新規のデータへ適用する場合は別途実行する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "\n",
    "# convert documents_training to Protobuf RecordIO format\n",
    "recordio_protobuf_serializer = RecordSerializer()\n",
    "fbuffer = recordio_protobuf_serializer.serialize(bags_of_words_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to S3 in bucket/prefix/train\n",
    "'''\n",
    "fname = 'lda.data'\n",
    "s3_object = os.path.join(prefix, 'test', fname)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(s3_object).upload_fileobj(fbuffer)\n",
    "\n",
    "s3_test_data = 's3://{}/{}'.format(bucket, s3_object)\n",
    "print('Uploaded data to S3: {}'.format(s3_test_data))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_path = f's3://{bucket}/{prefix}/output/lda_batch_transform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "transformer = lda.transformer(\n",
    "    instance_count = 1, \n",
    "    instance_type = 'ml.m5.xlarge', \n",
    "    output_path = output_path,\n",
    "    strategy = \"MultiRecord\",\n",
    "    max_payload = 1,\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data = s3_test_data, \n",
    "    data_type = \"S3Prefix\", \n",
    "    content_type = \"application/x-recordio-protobuf\", \n",
    "    split_type = \"RecordIO\",\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sagemaker.s3 import S3Downloader, s3_path_join\n",
    "\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = \"lda.data.out\"\n",
    "output_path = s3_path_join(output_path, output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path, '.')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open(output_file, 'r') as f:\n",
    "    output = json.load(f)\n",
    "    print(output)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Inference\n",
    "\n",
    "`deploy()`関数を使用して推論エンドポイントを作成します。推論を行うインスタンスタイプとインスタンスの初期数を指定します。    \n",
    "\n",
    "_**NOTE: 実際にサービス上でリアルタイムに使用するためには、文書に対して前処理（トークナイズ、単語の頻度表現）を行った上で推論エンドポイントへリクエストする必要があります。AWS LambdaやSageMakerの推論パイプラインなどを使用することができます**_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inference = lda.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'ml.m5.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(lda_inference.endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure data format (CSV, JSON, RECORDIO Protobu)\n",
    "lda_inference.serializer = CSVSerializer()\n",
    "lda_inference.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query endpoint\n",
    "results = lda_inference.predict(bags_of_words_test[:1])\n",
    "print(json.dumps(results, sort_keys=True, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's predict on the whole test set\n",
    "results = lda_inference.predict(bags_of_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "sagemaker.Session().delete_endpoint(lda_inference.endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
