{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1032ee6e",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Run a batch transform inference job with 🤗 Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e148674",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Run Batch Transform after training a model](#Run-Batch-Transform-after-training-a-model)  \n",
    "3. [Run Batch Transform Inference Job with a fine-tuned model using `jsonl`](#Run-Batch-Transform-Inference-Job-with-a-fine-tuned-model-using-jsonl)   \n",
    "\n",
    "HuggingFace Inference DLCsとAmazon SageMaker Python SDKを使用して、Transformersモデルでバッチ変換ジョブを実行します。\n",
    "このNotebookでは10,000以上のHugging Face Transformersモデルが存在するHuggingFace 🤗 [Hub](https://huggingface.co/models)にあるモデルを使用します。\n",
    "\n",
    "_**Note: 2021/09時点では日本語処理ライブラリの追加のため、あらかじめコンテナイメージを作成する必要があります。**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909707c6",
   "metadata": {},
   "source": [
    "## Run Batch Transform after training a model \n",
    "\n",
    "モデルを学習した後、[Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)を使用して、モデルを使った推論を実行できます。Batch Transformでは、S3に保存された推論データに対して、SageMakerがデータのダウンロード、予測の実行、結果のS3へのアップロードを行います。Batch Transformの詳細なドキュメントは[こちら](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)にあります。\n",
    "\n",
    "**HuggingFace estimator**を使用してモデルを学習した場合、`transformer()`メソッドを起動して、トレーニングジョブに基づいてモデルのバッチ変換ジョブを作成することができます。\n",
    "\n",
    "```python\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "batch_job.transform(\n",
    "    data='s3://s3-uri-to-batch-data',\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')\n",
    "```\n",
    "\n",
    "内容の詳細については、[API docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)をご参照ください。\n",
    "\n",
    "**このNotebookではモデルの学習は行わず、HuggingFace 🤗 [Hub](https://huggingface.co/models)にあるモデルを使用します。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e37bc",
   "metadata": {},
   "source": [
    "## IAM Role\n",
    "\n",
    "Note: IAMロールに以下の権限があることを確認してください:\n",
    "\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "\n",
    "ECRへイメージをpushするために、IAMにAmazonEC2ContainerRegistryFullAccessの権限を付与する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd11bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"datasets==1.11\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc58a0",
   "metadata": {},
   "source": [
    "# Run Batch Transform Inference Job with a fine-tuned model using `jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22d912",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "\n",
    "registry_name = 'huggingface-japanese-inference-gpu'\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = sess.region_name\n",
    "\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c73d97",
   "metadata": {},
   "source": [
    "### Build a HuggingFace Docker container and push it to Amazon ECR\n",
    "\n",
    "Dockerfileは[こちら](https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/inference/docker/1.7/py3/cu110/Dockerfile.gpu)を一部修正し、使用しています。    \n",
    "\n",
    "変更点\n",
    "\n",
    "- 16行目: `TRANSFORMERS_VERSION` → `TRANSFORMERS_VERSION=4.6.1`\n",
    "- 111行目: `transformers[sentencepiece]` → `transformers[ja]`\n",
    "\n",
    "サンプルはGPUインスタンス用となっており、CPUインスタンス上で推論したい場合は[こちら](https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/inference/docker/1.7/py3/Dockerfile.cpu)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "%cd container_gpu\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ffdd1",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Amazon の商品レビューデータセットは[Registry of Open Data on AWS](https://registry.opendata.aws/)で公開されており、 以下からダウンロード可能です。    \n",
    "このノートブックでは、日本語のデータセットをダウンロードします。\n",
    "\n",
    "- データセットの概要    \n",
    "https://registry.opendata.aws/amazon-reviews/\n",
    "\n",
    "- 日本語のデータセット(readme.htmlからたどることができます）    \n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
    "\n",
    "以下では、データをダウンロードして解凍 (unzip) します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "download_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\" \n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File {} already exists. Skipped download.\".format(file_name))\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    print(\"File downloaded: {}\".format(file_path))\n",
    "    \n",
    "if os.path.exists(tsv_file_path):\n",
    "    print(\"File {} already exists. Skipped unzip.\".format(tsv_file_name))\n",
    "else:\n",
    "    with gzip.open(file_path, mode='rb') as fin:\n",
    "        with open(tsv_file_path, 'wb') as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "            print(\"File uznipped: {}\".format(tsv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1578f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep ='\\t')\n",
    "df_pos_neg = df.loc[:, [\"star_rating\", \"review_body\"]]\n",
    "df_pos_neg = df_pos_neg[df_pos_neg.star_rating != 3]\n",
    "df_pos_neg.loc[df_pos_neg.star_rating < 3, \"star_rating\"] = 0\n",
    "df_pos_neg.loc[df_pos_neg.star_rating > 3, \"star_rating\"] = 1\n",
    "\n",
    "print(df_pos_neg.shape)\n",
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# デモ用にサンプリングしてデータを小さくします\n",
    "df_pos_neg = df_pos_neg.sample(2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストのカラム名を'review_body'から'inputs'へ変更してcsvで保存します\n",
    "df_pos_neg = df_pos_neg.rename(columns={'review_body': 'inputs'})\n",
    "df_pos_neg.loc[:, 'inputs'].to_csv('review_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd3455",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "先ほど保存した`csv`をバッチ変換ジョブで使用するには、`jsonl`ファイルに変換して、S3にアップロードする必要があります。テキストは複雑な構造をしているので、バッチ変換ジョブでサポートしているのは `jsonl` ファイルだけです。前処理として、改行コード（`<br />`）を削除して、文字数を500文字以内にしています。\n",
    "\n",
    "バッチ変換ジョブを行う際には、`inputs`のトークン数が`max_length`に収まっていることを確認する必要があります。ここでは簡便的に最大文字数を500文字にして`max_length`内に収めています（これから使用するモデルの`max_length`は512）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e23136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# datset files\n",
    "dataset_csv_file=\"review_data.csv\"\n",
    "dataset_jsonl_file=\"review_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    reader = csv.DictReader(infile, )\n",
    "    for row in reader:\n",
    "        row[\"inputs\"] = row[\"inputs\"].replace(\"<br />\",\"\")[:500]\n",
    "        json.dump(row, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7997260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file, input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6cc02",
   "metadata": {},
   "source": [
    "`review_data.jsonl`は以下のようになります。\n",
    "\n",
    "```json\n",
    "{\"inputs\": \"確かに映像はきれいですが、話の内容的には、「……」という感じです。いかにもワイヤーというのが、見ていて飽き飽きしてしまいました。\"}\n",
    "{\"inputs\": \"天使の街で、死の天使が微笑んだ。1940年代後半ロサンゼルスは、華やかな顔のその裏で暴力がはびこり裏社会の頂点に立った男が、悪の限りを尽くしていた。その男の名はミッキー・コーエン、司法 , 警察までもが彼の手中にあり、腐敗しきっていた。そんな時市警察の異端児たちが密命を受け、街を取り戻すため、命を懸けて裏社会に戦いを挑んだ。人の装いや暮らし , 街並み、そして街を流す車に時代の色と香りが溢れます。日が沈むと・・・夜な夜な人々が集う、華やかでそして危険な社交場。昼と夜、表と裏、光と影、裏社会の勢いがそんな垣根も越えて行く時代。でも、喰うか喰われるか , 生きるか死ぬかの (筈の) 戦いが始まると、少々拍子抜け。ギリギリした緊張感や、胃液が上がって来る様な緊迫感に欠ける感じがする。その点が一番惜しい。金と血の匂いが好きなミッキー・コーエンを、ショーン・ペンがどんぴしゃりと演じています。ライアン・ゴズリングは、あの時代背景の中で映えまくっています。当時の装いを見事に着こなし、荒んで見せても、甘く繊細な雰囲気が漂います。何を期待したのか、それによって印象がだいぶ変わるかもしれません。私自身肩透\"}\n",
    "{\"inputs\": \"2008年にリリースされた、USのヘヴィロック・バンドの3rdアルバム。日本ではあまり知られていないかもしれませんが、本国アメリカでは知名度のあるバンドです。このバンドの主役はヴォーカルのブレント・スミスの熱い歌唱だと思います。男臭いエモーショナルな歌声で、とても良い声をしています。曲は、疾走感のあるハードなロックナンバーから歌声で聴かせるバラードまであり、それぞれ曲の質が高いと思います。キャッチーさがあって聴きやすいのも良いです。1曲目の「Devour」は繰り返されるフレーズとサビが印象的なノリの良いハードなナンバーで、私は初めて聴いた時はDISTURBEDかと思いました・・・。久しぶりに聴いたのですが、今聴いても良いと思うアルバムでした。\"}\n",
    "{\"inputs\": \"まさにストリートレース!!この映画を越える作品は無いんじゃないかな、何よりスピード感 最初のドラッグレース シーンは迫力満点、後半でエクリプスがNosを噴射したシーンは画面が揺れ 画面効果が凄い、さすがロブ・コーエン監督!!車はどちらかと言うと 見た目はスポコンで 日本車がメイン!!インテグラ シビック JZA80スープラ FD3S S14シルビア R33 GT-R エクリプス日本からは Bomexなど有名ショップも協力していて出来がGood!!!最後には1970 Dodge Chargerが出演 最後までスピード感があり ストーリー性も完璧です。\"}\n",
    "{\"inputs\": \"各国版が出ていて、共通のテキストとしては最適だと思います。ただ、内容がちょっと古いことと、（今時ハンサムはない）出てくる人の名前がとても読みにくい。\"}\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73e846",
   "metadata": {},
   "source": [
    "## Create Inference Transformer to run the batch job\n",
    "\n",
    "_これは実験的な機能で、エンドポイントが作成された後にモデルが読み込まれるようになっています。これにより、モデルが10GBを超える場合などでエラーが発生する可能性があります_\n",
    "\n",
    "🤗 HubからSageMakerにモデルを直接デプロイするには、`HuggingFaceModel`の作成時に2つの環境変数を定義する必要があります:\n",
    "\n",
    "- `HF_MODEL_ID`: SageMakerエンドポイントを作成する際に、[huggingface.co/models](http://huggingface.co/models) から自動的にロードされるモデルIDを定義します。🤗 Hubは10,000以上のモデルを提供しており、この環境変数で利用できます。\n",
    "\n",
    "- `HF_TASK`: 使用する🤗 Transformersのパイプラインのタスクを定義します。タスクの完全なリストは [ここ](https://huggingface.co/transformers/main_classes/pipelines.html) にあります。\n",
    "\n",
    "このサンプルでは、🤗 Hubから https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363 をデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'abhishek/autonlp-japanese-sentiment-59363', # model_id from hf.co/models\n",
    "  'HF_TASK':'text-classification' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image,\n",
    "    env=hub,\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    "    #transformers_version=\"4.6\", # transformers version used\n",
    "    #pytorch_version=\"1.7\", # pytorch version used\n",
    "    #py_version=\"py36\", # python version of the DLC\n",
    ")\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916de4ff",
   "metadata": {},
   "source": [
    "## Download-predict-file-from-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join(output_s3_path,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67112e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
