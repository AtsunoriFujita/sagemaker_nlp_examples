{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1032ee6e",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Run a batch transform inference job with ğŸ¤— Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e148674",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Run Batch Transform after training a model](#Run-Batch-Transform-after-training-a-model)  \n",
    "3. [Run Batch Transform Inference Job with a fine-tuned model using `jsonl`](#Run-Batch-Transform-Inference-Job-with-a-fine-tuned-model-using-jsonl)   \n",
    "\n",
    "HuggingFace Inference DLCsã¨Amazon SageMaker Python SDKã‚’ä½¿ç”¨ã—ã¦ã€Transformersãƒ¢ãƒ‡ãƒ«ã§ãƒãƒƒãƒå¤‰æ›ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n",
    "ã“ã®Notebookã§ã¯10,000ä»¥ä¸Šã®Hugging Face Transformersãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹HuggingFace ğŸ¤— [Hub](https://huggingface.co/models)ã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "_**Note: 2021/09æ™‚ç‚¹ã§ã¯æ—¥æœ¬èªå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è¿½åŠ ã®ãŸã‚ã€ã‚ã‚‰ã‹ã˜ã‚ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909707c6",
   "metadata": {},
   "source": [
    "## Run Batch Transform after training a model \n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ãŸå¾Œã€[Amazon SageMaker Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html)ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ãŸæ¨è«–ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚Batch Transformã§ã¯ã€S3ã«ä¿å­˜ã•ã‚ŒãŸæ¨è«–ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€SageMakerãŒãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€äºˆæ¸¬ã®å®Ÿè¡Œã€çµæœã®S3ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’è¡Œã„ã¾ã™ã€‚Batch Transformã®è©³ç´°ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯[ã“ã¡ã‚‰](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html)ã«ã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "**HuggingFace estimator**ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ãŸå ´åˆã€`transformer()`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’èµ·å‹•ã—ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¸ãƒ§ãƒ–ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒãƒå¤‰æ›ã‚¸ãƒ§ãƒ–ã‚’ä½œæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "```python\n",
    "batch_job = huggingface_estimator.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.c5.2xlarge',\n",
    "    strategy='SingleRecord')\n",
    "\n",
    "batch_job.transform(\n",
    "    data='s3://s3-uri-to-batch-data',\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')\n",
    "```\n",
    "\n",
    "å†…å®¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[API docs](https://sagemaker.readthedocs.io/en/stable/overview.html#sagemaker-batch-transform)ã‚’ã”å‚ç…§ãã ã•ã„ã€‚\n",
    "\n",
    "**ã“ã®Notebookã§ã¯ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¯è¡Œã‚ãšã€HuggingFace ğŸ¤— [Hub](https://huggingface.co/models)ã«ã‚ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e37bc",
   "metadata": {},
   "source": [
    "## IAM Role\n",
    "\n",
    "Note: IAMãƒ­ãƒ¼ãƒ«ã«ä»¥ä¸‹ã®æ¨©é™ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\n",
    "\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "\n",
    "ECRã¸ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’pushã™ã‚‹ãŸã‚ã«ã€IAMã«AmazonEC2ContainerRegistryFullAccessã®æ¨©é™ã‚’ä»˜ä¸ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd11bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"datasets==1.11\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fc58a0",
   "metadata": {},
   "source": [
    "# Run Batch Transform Inference Job with a fine-tuned model using `jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "\n",
    "# get the s3 bucket\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session_bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22d912",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d3e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "\n",
    "registry_name = 'huggingface-japanese-inference-gpu'\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = sess.region_name\n",
    "\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c73d97",
   "metadata": {},
   "source": [
    "### Build a HuggingFace Docker container and push it to Amazon ECR\n",
    "\n",
    "Dockerfileã¯[ã“ã¡ã‚‰](https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/inference/docker/1.7/py3/cu110/Dockerfile.gpu)ã‚’ä¸€éƒ¨ä¿®æ­£ã—ã€ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚    \n",
    "\n",
    "å¤‰æ›´ç‚¹\n",
    "\n",
    "- 16è¡Œç›®: `TRANSFORMERS_VERSION` â†’ `TRANSFORMERS_VERSION=4.6.1`\n",
    "- 111è¡Œç›®: `transformers[sentencepiece]` â†’ `transformers[ja]`\n",
    "\n",
    "ã‚µãƒ³ãƒ—ãƒ«ã¯GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”¨ã¨ãªã£ã¦ãŠã‚Šã€CPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§æ¨è«–ã—ãŸã„å ´åˆã¯[ã“ã¡ã‚‰](https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/inference/docker/1.7/py3/Dockerfile.cpu)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d16c390",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "%cd container_gpu\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182ffdd1",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Amazon ã®å•†å“ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯[Registry of Open Data on AWS](https://registry.opendata.aws/)ã§å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€ ä»¥ä¸‹ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯èƒ½ã§ã™ã€‚    \n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n",
    "\n",
    "- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¦‚è¦    \n",
    "https://registry.opendata.aws/amazon-reviews/\n",
    "\n",
    "- æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(readme.htmlã‹ã‚‰ãŸã©ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰    \n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
    "\n",
    "ä»¥ä¸‹ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦è§£å‡ (unzip) ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d4607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "download_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\" \n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File {} already exists. Skipped download.\".format(file_name))\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    print(\"File downloaded: {}\".format(file_path))\n",
    "    \n",
    "if os.path.exists(tsv_file_path):\n",
    "    print(\"File {} already exists. Skipped unzip.\".format(tsv_file_name))\n",
    "else:\n",
    "    with gzip.open(file_path, mode='rb') as fin:\n",
    "        with open(tsv_file_path, 'wb') as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "            print(\"File uznipped: {}\".format(tsv_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1578f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep ='\\t')\n",
    "df_pos_neg = df.loc[:, [\"star_rating\", \"review_body\"]]\n",
    "df_pos_neg = df_pos_neg[df_pos_neg.star_rating != 3]\n",
    "df_pos_neg.loc[df_pos_neg.star_rating < 3, \"star_rating\"] = 0\n",
    "df_pos_neg.loc[df_pos_neg.star_rating > 3, \"star_rating\"] = 1\n",
    "\n",
    "print(df_pos_neg.shape)\n",
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ‡ãƒ¢ç”¨ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’å°ã•ãã—ã¾ã™\n",
    "df_pos_neg = df_pos_neg.sample(2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒ©ãƒ åã‚’'review_body'ã‹ã‚‰'inputs'ã¸å¤‰æ›´ã—ã¦csvã§ä¿å­˜ã—ã¾ã™\n",
    "df_pos_neg = df_pos_neg.rename(columns={'review_body': 'inputs'})\n",
    "df_pos_neg.loc[:, 'inputs'].to_csv('review_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd3455",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "å…ˆã»ã©ä¿å­˜ã—ãŸ`csv`ã‚’ãƒãƒƒãƒå¤‰æ›ã‚¸ãƒ§ãƒ–ã§ä½¿ç”¨ã™ã‚‹ã«ã¯ã€`jsonl`ãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›ã—ã¦ã€S3ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã¯è¤‡é›‘ãªæ§‹é€ ã‚’ã—ã¦ã„ã‚‹ã®ã§ã€ãƒãƒƒãƒå¤‰æ›ã‚¸ãƒ§ãƒ–ã§ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã®ã¯ `jsonl` ãƒ•ã‚¡ã‚¤ãƒ«ã ã‘ã§ã™ã€‚å‰å‡¦ç†ã¨ã—ã¦ã€æ”¹è¡Œã‚³ãƒ¼ãƒ‰ï¼ˆ`<br />`ï¼‰ã‚’å‰Šé™¤ã—ã¦ã€æ–‡å­—æ•°ã‚’500æ–‡å­—ä»¥å†…ã«ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ãƒãƒƒãƒå¤‰æ›ã‚¸ãƒ§ãƒ–ã‚’è¡Œã†éš›ã«ã¯ã€`inputs`ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ`max_length`ã«åã¾ã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ç°¡ä¾¿çš„ã«æœ€å¤§æ–‡å­—æ•°ã‚’500æ–‡å­—ã«ã—ã¦`max_length`å†…ã«åã‚ã¦ã„ã¾ã™ï¼ˆã“ã‚Œã‹ã‚‰ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®`max_length`ã¯512ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e23136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# datset files\n",
    "dataset_csv_file=\"review_data.csv\"\n",
    "dataset_jsonl_file=\"review_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bd4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_csv_file, \"r+\") as infile, open(dataset_jsonl_file, \"w+\") as outfile:\n",
    "    reader = csv.DictReader(infile, )\n",
    "    for row in reader:\n",
    "        row[\"inputs\"] = row[\"inputs\"].replace(\"<br />\",\"\")[:500]\n",
    "        json.dump(row, outfile, ensure_ascii=False)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7997260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploads a given file to S3.\n",
    "input_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/input\")\n",
    "output_s3_path = s3_path_join(\"s3://\", sagemaker_session_bucket, \"batch_transform/output\")\n",
    "s3_file_uri = S3Uploader.upload(dataset_jsonl_file, input_s3_path)\n",
    "\n",
    "print(f\"{dataset_jsonl_file} uploaded to {s3_file_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b6cc02",
   "metadata": {},
   "source": [
    "`review_data.jsonl`ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "```json\n",
    "{\"inputs\": \"ç¢ºã‹ã«æ˜ åƒã¯ãã‚Œã„ã§ã™ãŒã€è©±ã®å†…å®¹çš„ã«ã¯ã€ã€Œâ€¦â€¦ã€ã¨ã„ã†æ„Ÿã˜ã§ã™ã€‚ã„ã‹ã«ã‚‚ãƒ¯ã‚¤ãƒ¤ãƒ¼ã¨ã„ã†ã®ãŒã€è¦‹ã¦ã„ã¦é£½ãé£½ãã—ã¦ã—ã¾ã„ã¾ã—ãŸã€‚\"}\n",
    "{\"inputs\": \"å¤©ä½¿ã®è¡—ã§ã€æ­»ã®å¤©ä½¿ãŒå¾®ç¬‘ã‚“ã ã€‚1940å¹´ä»£å¾ŒåŠãƒ­ã‚µãƒ³ã‚¼ãƒ«ã‚¹ã¯ã€è¯ã‚„ã‹ãªé¡”ã®ãã®è£ã§æš´åŠ›ãŒã¯ã³ã“ã‚Šè£ç¤¾ä¼šã®é ‚ç‚¹ã«ç«‹ã£ãŸç”·ãŒã€æ‚ªã®é™ã‚Šã‚’å°½ãã—ã¦ã„ãŸã€‚ãã®ç”·ã®åã¯ãƒŸãƒƒã‚­ãƒ¼ãƒ»ã‚³ãƒ¼ã‚¨ãƒ³ã€å¸æ³• , è­¦å¯Ÿã¾ã§ã‚‚ãŒå½¼ã®æ‰‹ä¸­ã«ã‚ã‚Šã€è…æ•—ã—ãã£ã¦ã„ãŸã€‚ãã‚“ãªæ™‚å¸‚è­¦å¯Ÿã®ç•°ç«¯å…ãŸã¡ãŒå¯†å‘½ã‚’å—ã‘ã€è¡—ã‚’å–ã‚Šæˆ»ã™ãŸã‚ã€å‘½ã‚’æ‡¸ã‘ã¦è£ç¤¾ä¼šã«æˆ¦ã„ã‚’æŒ‘ã‚“ã ã€‚äººã®è£…ã„ã‚„æš®ã‚‰ã— , è¡—ä¸¦ã¿ã€ãã—ã¦è¡—ã‚’æµã™è»Šã«æ™‚ä»£ã®è‰²ã¨é¦™ã‚ŠãŒæº¢ã‚Œã¾ã™ã€‚æ—¥ãŒæ²ˆã‚€ã¨ãƒ»ãƒ»ãƒ»å¤œãªå¤œãªäººã€…ãŒé›†ã†ã€è¯ã‚„ã‹ã§ãã—ã¦å±é™ºãªç¤¾äº¤å ´ã€‚æ˜¼ã¨å¤œã€è¡¨ã¨è£ã€å…‰ã¨å½±ã€è£ç¤¾ä¼šã®å‹¢ã„ãŒãã‚“ãªå£æ ¹ã‚‚è¶Šãˆã¦è¡Œãæ™‚ä»£ã€‚ã§ã‚‚ã€å–°ã†ã‹å–°ã‚ã‚Œã‚‹ã‹ , ç”Ÿãã‚‹ã‹æ­»ã¬ã‹ã® (ç­ˆã®) æˆ¦ã„ãŒå§‹ã¾ã‚‹ã¨ã€å°‘ã€…æ‹å­æŠœã‘ã€‚ã‚®ãƒªã‚®ãƒªã—ãŸç·Šå¼µæ„Ÿã‚„ã€èƒƒæ¶²ãŒä¸ŠãŒã£ã¦æ¥ã‚‹æ§˜ãªç·Šè¿«æ„Ÿã«æ¬ ã‘ã‚‹æ„Ÿã˜ãŒã™ã‚‹ã€‚ãã®ç‚¹ãŒä¸€ç•ªæƒœã—ã„ã€‚é‡‘ã¨è¡€ã®åŒ‚ã„ãŒå¥½ããªãƒŸãƒƒã‚­ãƒ¼ãƒ»ã‚³ãƒ¼ã‚¨ãƒ³ã‚’ã€ã‚·ãƒ§ãƒ¼ãƒ³ãƒ»ãƒšãƒ³ãŒã©ã‚“ã´ã—ã‚ƒã‚Šã¨æ¼”ã˜ã¦ã„ã¾ã™ã€‚ãƒ©ã‚¤ã‚¢ãƒ³ãƒ»ã‚´ã‚ºãƒªãƒ³ã‚°ã¯ã€ã‚ã®æ™‚ä»£èƒŒæ™¯ã®ä¸­ã§æ˜ ãˆã¾ãã£ã¦ã„ã¾ã™ã€‚å½“æ™‚ã®è£…ã„ã‚’è¦‹äº‹ã«ç€ã“ãªã—ã€è’ã‚“ã§è¦‹ã›ã¦ã‚‚ã€ç”˜ãç¹Šç´°ãªé›°å›²æ°—ãŒæ¼‚ã„ã¾ã™ã€‚ä½•ã‚’æœŸå¾…ã—ãŸã®ã‹ã€ãã‚Œã«ã‚ˆã£ã¦å°è±¡ãŒã ã„ã¶å¤‰ã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ç§è‡ªèº«è‚©é€\"}\n",
    "{\"inputs\": \"2008å¹´ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸã€USã®ãƒ˜ãƒ´ã‚£ãƒ­ãƒƒã‚¯ãƒ»ãƒãƒ³ãƒ‰ã®3rdã‚¢ãƒ«ãƒãƒ ã€‚æ—¥æœ¬ã§ã¯ã‚ã¾ã‚ŠçŸ¥ã‚‰ã‚Œã¦ã„ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€æœ¬å›½ã‚¢ãƒ¡ãƒªã‚«ã§ã¯çŸ¥ååº¦ã®ã‚ã‚‹ãƒãƒ³ãƒ‰ã§ã™ã€‚ã“ã®ãƒãƒ³ãƒ‰ã®ä¸»å½¹ã¯ãƒ´ã‚©ãƒ¼ã‚«ãƒ«ã®ãƒ–ãƒ¬ãƒ³ãƒˆãƒ»ã‚¹ãƒŸã‚¹ã®ç†±ã„æ­Œå”±ã ã¨æ€ã„ã¾ã™ã€‚ç”·è‡­ã„ã‚¨ãƒ¢ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãªæ­Œå£°ã§ã€ã¨ã¦ã‚‚è‰¯ã„å£°ã‚’ã—ã¦ã„ã¾ã™ã€‚æ›²ã¯ã€ç–¾èµ°æ„Ÿã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ãªãƒ­ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‹ã‚‰æ­Œå£°ã§è´ã‹ã›ã‚‹ãƒãƒ©ãƒ¼ãƒ‰ã¾ã§ã‚ã‚Šã€ãã‚Œãã‚Œæ›²ã®è³ªãŒé«˜ã„ã¨æ€ã„ã¾ã™ã€‚ã‚­ãƒ£ãƒƒãƒãƒ¼ã•ãŒã‚ã£ã¦è´ãã‚„ã™ã„ã®ã‚‚è‰¯ã„ã§ã™ã€‚1æ›²ç›®ã®ã€ŒDevourã€ã¯ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºã¨ã‚µãƒ“ãŒå°è±¡çš„ãªãƒãƒªã®è‰¯ã„ãƒãƒ¼ãƒ‰ãªãƒŠãƒ³ãƒãƒ¼ã§ã€ç§ã¯åˆã‚ã¦è´ã„ãŸæ™‚ã¯DISTURBEDã‹ã¨æ€ã„ã¾ã—ãŸãƒ»ãƒ»ãƒ»ã€‚ä¹…ã—ã¶ã‚Šã«è´ã„ãŸã®ã§ã™ãŒã€ä»Šè´ã„ã¦ã‚‚è‰¯ã„ã¨æ€ã†ã‚¢ãƒ«ãƒãƒ ã§ã—ãŸã€‚\"}\n",
    "{\"inputs\": \"ã¾ã•ã«ã‚¹ãƒˆãƒªãƒ¼ãƒˆãƒ¬ãƒ¼ã‚¹!!ã“ã®æ˜ ç”»ã‚’è¶Šãˆã‚‹ä½œå“ã¯ç„¡ã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã€ä½•ã‚ˆã‚Šã‚¹ãƒ”ãƒ¼ãƒ‰æ„Ÿ æœ€åˆã®ãƒ‰ãƒ©ãƒƒã‚°ãƒ¬ãƒ¼ã‚¹ ã‚·ãƒ¼ãƒ³ã¯è¿«åŠ›æº€ç‚¹ã€å¾ŒåŠã§ã‚¨ã‚¯ãƒªãƒ—ã‚¹ãŒNosã‚’å™´å°„ã—ãŸã‚·ãƒ¼ãƒ³ã¯ç”»é¢ãŒæºã‚Œ ç”»é¢åŠ¹æœãŒå‡„ã„ã€ã•ã™ãŒãƒ­ãƒ–ãƒ»ã‚³ãƒ¼ã‚¨ãƒ³ç›£ç£!!è»Šã¯ã©ã¡ã‚‰ã‹ã¨è¨€ã†ã¨ è¦‹ãŸç›®ã¯ã‚¹ãƒã‚³ãƒ³ã§ æ—¥æœ¬è»ŠãŒãƒ¡ã‚¤ãƒ³!!ã‚¤ãƒ³ãƒ†ã‚°ãƒ© ã‚·ãƒ“ãƒƒã‚¯ JZA80ã‚¹ãƒ¼ãƒ—ãƒ© FD3S S14ã‚·ãƒ«ãƒ“ã‚¢ R33 GT-R ã‚¨ã‚¯ãƒªãƒ—ã‚¹æ—¥æœ¬ã‹ã‚‰ã¯ Bomexãªã©æœ‰åã‚·ãƒ§ãƒƒãƒ—ã‚‚å”åŠ›ã—ã¦ã„ã¦å‡ºæ¥ãŒGood!!!æœ€å¾Œã«ã¯1970 Dodge ChargerãŒå‡ºæ¼” æœ€å¾Œã¾ã§ã‚¹ãƒ”ãƒ¼ãƒ‰æ„ŸãŒã‚ã‚Š ã‚¹ãƒˆãƒ¼ãƒªãƒ¼æ€§ã‚‚å®Œç’§ã§ã™ã€‚\"}\n",
    "{\"inputs\": \"å„å›½ç‰ˆãŒå‡ºã¦ã„ã¦ã€å…±é€šã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ã¯æœ€é©ã ã¨æ€ã„ã¾ã™ã€‚ãŸã ã€å†…å®¹ãŒã¡ã‚‡ã£ã¨å¤ã„ã“ã¨ã¨ã€ï¼ˆä»Šæ™‚ãƒãƒ³ã‚µãƒ ã¯ãªã„ï¼‰å‡ºã¦ãã‚‹äººã®åå‰ãŒã¨ã¦ã‚‚èª­ã¿ã«ãã„ã€‚\"}\n",
    "....\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae73e846",
   "metadata": {},
   "source": [
    "## Create Inference Transformer to run the batch job\n",
    "\n",
    "_ã“ã‚Œã¯å®Ÿé¨“çš„ãªæ©Ÿèƒ½ã§ã€ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒä½œæˆã•ã‚ŒãŸå¾Œã«ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒ10GBã‚’è¶…ãˆã‚‹å ´åˆãªã©ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™_\n",
    "\n",
    "ğŸ¤— Hubã‹ã‚‰SageMakerã«ãƒ¢ãƒ‡ãƒ«ã‚’ç›´æ¥ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã«ã¯ã€`HuggingFaceModel`ã®ä½œæˆæ™‚ã«2ã¤ã®ç’°å¢ƒå¤‰æ•°ã‚’å®šç¾©ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™:\n",
    "\n",
    "- `HF_MODEL_ID`: SageMakerã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹éš›ã«ã€[huggingface.co/models](http://huggingface.co/models) ã‹ã‚‰è‡ªå‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«IDã‚’å®šç¾©ã—ã¾ã™ã€‚ğŸ¤— Hubã¯10,000ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã—ã¦ãŠã‚Šã€ã“ã®ç’°å¢ƒå¤‰æ•°ã§åˆ©ç”¨ã§ãã¾ã™ã€‚\n",
    "\n",
    "- `HF_TASK`: ä½¿ç”¨ã™ã‚‹ğŸ¤— Transformersã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã‚¿ã‚¹ã‚¯ã‚’å®šç¾©ã—ã¾ã™ã€‚ã‚¿ã‚¹ã‚¯ã®å®Œå…¨ãªãƒªã‚¹ãƒˆã¯ [ã“ã“](https://huggingface.co/transformers/main_classes/pipelines.html) ã«ã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ã€ğŸ¤— Hubã‹ã‚‰ https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363 ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e11eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'abhishek/autonlp-japanese-sentiment-59363', # model_id from hf.co/models\n",
    "  'HF_TASK':'text-classification' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image,\n",
    "    env=hub,\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    "    #transformers_version=\"4.6\", # transformers version used\n",
    "    #pytorch_version=\"1.7\", # pytorch version used\n",
    "    #py_version=\"py36\", # python version of the DLC\n",
    ")\n",
    "\n",
    "# create Transformer to run our batch job\n",
    "batch_job = huggingface_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    output_path=output_s3_path, # we are using the same s3 path to save the output with the input\n",
    "    strategy='SingleRecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starts batch transform job and uses s3 data as input\n",
    "batch_job.transform(\n",
    "    data=s3_file_uri,\n",
    "    content_type='application/json',    \n",
    "    split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916de4ff",
   "metadata": {},
   "source": [
    "## Download-predict-file-from-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35ae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "from ast import literal_eval\n",
    "\n",
    "# creating s3 uri for result file -> input file + .out\n",
    "output_file = f\"{dataset_jsonl_file}.out\"\n",
    "output_path = s3_path_join(output_s3_path,output_file)\n",
    "\n",
    "# download file\n",
    "S3Downloader.download(output_path,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67112e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_result = []\n",
    "with open(output_file) as f:\n",
    "    for line in f:\n",
    "        # converts jsonline array to normal array\n",
    "        line = \"[\" + line.replace(\"[\",\"\").replace(\"]\",\",\") + \"]\"\n",
    "        batch_transform_result = literal_eval(line) \n",
    "        \n",
    "# print results \n",
    "print(batch_transform_result[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18bb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07ae73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
