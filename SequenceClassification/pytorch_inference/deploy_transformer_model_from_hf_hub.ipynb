{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d780f9",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Deploy 🤗 Transformers for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f8dc64",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)    \n",
    "    a. [Build a HuggingFace Docker container and push it to Amazon ECR](#Build-a-HuggingFace-Docker-container-and-push-it-to-Amazon-ECR)    \n",
    "    b. [Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference](#Deploy-one-of-the-10-000+-Hugging-Face-Transformers-to-Amazon-SageMaker-for-Inference)   \n",
    "\n",
    "\n",
    "HuggingFace Inference DLCsとAmazon SageMaker Python SDKを使用して、Transformersモデルをデプロイします。    \n",
    "このNotebookでは10,000以上のHugging Face Transformersモデルが存在するHuggingFace 🤗 [Hub](https://huggingface.co/models)からAmazon SageMakerに直接デプロイして推論します。\n",
    "\n",
    "_**Note: 2021/08時点では日本語処理ライブラリの追加のため、あらかじめコンテナイメージを作成する必要があります。**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f5a77",
   "metadata": {},
   "source": [
    "## API - [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)\n",
    "\n",
    "\n",
    "`transformers pipelines`を利用して、`pipelines`の全機能を簡単に利用できるAPIを設計しました。APIは[🤗 Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html)のAPIを参考にしています。つまり、入力は `inputs` keyで定義する必要があり、サポートされている `pipelines` のパラメータを追加したい場合には `parameters` keyで追加することができます。以下にリクエストの例を示します。\n",
    "\n",
    "**text-classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "**question-answering request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": {\n",
    "\t\t\"question\": \"What is used for inference?\",\n",
    "\t\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "**zero-shot classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"candidate_labels\": [\n",
    "\t\t\t\"refund\",\n",
    "\t\t\t\"legal\",\n",
    "\t\t\t\"faq\"\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8851e7",
   "metadata": {},
   "source": [
    "## IAM Role\n",
    "_**Note**: IAMロールに以下の権限があることを確認してください:_\n",
    "\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "\n",
    "ECRへイメージをpushするために、IAMに`AmazonEC2ContainerRegistryFullAccess`の権限を付与する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"sagemaker>=2.48.1\" \"transformers[ja]==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac82f99",
   "metadata": {},
   "source": [
    "## Create an Amazon ECR registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8ab8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "\n",
    "registry_name = 'huggingface-japanese-inference-cpu'\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = sess.region_name\n",
    "\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1809c894",
   "metadata": {},
   "source": [
    "## Build a HuggingFace Docker container and push it to Amazon ECR\n",
    "\n",
    "Dockerfileは[こちら](https://github.com/aws/deep-learning-containers/blob/master/huggingface/pytorch/inference/docker/1.7/py3/Dockerfile.cpu)を一部修正し、使用しています。    \n",
    "\n",
    "変更点\n",
    "\n",
    "- 19行目: `TRANSFORMERS_VERSION` → `TRANSFORMERS_VERSION=4.6.1`\n",
    "- 116行目: `transformers[sentencepiece]` → `transformers[ja]`\n",
    "\n",
    "サンプルはCPUインスタンス用となっており、GPUインスタンス上で推論したい場合は[こちら](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch/inference/docker/1.7/py3/cu110)を参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c1626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "%cd container_cpu\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84614be0",
   "metadata": {},
   "source": [
    "## Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference\n",
    "\n",
    "_これは実験的な機能で、エンドポイントが作成された後にモデルが読み込まれるようになっています。これにより、モデルが10GBを超える場合などでエラーが発生する可能性があります_\n",
    "\n",
    "🤗 HubからSageMakerにモデルを直接デプロイするには、`HuggingFaceModel`の作成時に2つの環境変数を定義する必要があります:\n",
    "\n",
    "- `HF_MODEL_ID`: SageMakerエンドポイントを作成する際に、[huggingface.co/models](http://huggingface.co/models) から自動的にロードされるモデルIDを定義します。🤗 Hubは10,000以上のモデルを提供しており、この環境変数で利用できます。\n",
    "\n",
    "- `HF_TASK`: 使用する🤗 Transformersのパイプラインのタスクを定義します。タスクの完全なリストは [ここ](https://huggingface.co/transformers/main_classes/pipelines.html) にあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28e2d5",
   "metadata": {},
   "source": [
    "このサンプルでは、🤗 Hubから https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363 をデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d023fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'abhishek/autonlp-japanese-sentiment-59363', # model_id from hf.co/models\n",
    "  'HF_TASK':'text-classification' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    image_uri=image,\n",
    "    env=hub,\n",
    "    role=role, # iam role with permissions to create an Endpoint\n",
    "    #transformers_version=\"4.6\", # transformers version used\n",
    "    #pytorch_version=\"1.7\", # pytorch version used\n",
    "    #py_version=\"py36\", # python version of the DLC\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f14498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "    \"inputs\": 'ハワイアンの心和む音楽の中、ちょっとシリアスなドラマが展開していきます。音楽の力ってすごいな、って思いました。'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4994a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf442d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
