{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "947947f9",
   "metadata": {},
   "source": [
    "# 🤗 Transformers on AWS Lambda container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5a0c36",
   "metadata": {},
   "source": [
    "学習したモデルはコンテナを使ってAWS Lambda上で推論することも可能です。ここではその方法を紹介します。\n",
    "\n",
    "**このNotebookではモデルの学習は行わず、HuggingFace 🤗 [Hub](https://huggingface.co/models)にあるモデルを使用します。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101b784",
   "metadata": {},
   "source": [
    "## IAM Role\n",
    "\n",
    "Note: IAMロールに以下の権限があることを確認してください:\n",
    "\n",
    "- AmazonSageMakerFullAccess\n",
    "- AmazonS3FullAccess\n",
    "- AmazonEC2ContainerRegistryFullAccess\n",
    "- AWSLambda_FullAccess\n",
    "\n",
    "ECRとLambdaを使用するために、`AmazonEC2ContainerRegistryFullAccess`と`AWSLambda_FullAccess`が必要になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff528ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"torch>=1.7.1\" \"transformers[ja]==4.6.0\" \"datasets==1.11\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4654a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './lambda-docker/model'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f5f401",
   "metadata": {},
   "source": [
    "コンテナデプロイのためにModelとTokenizerをダウンロードします（あらかじめ作成したModelとTokenizerを使用する場合は`OUTPUT_DIR`に配置してください）。    \n",
    "このサンプルでは、🤗 Hubから https://huggingface.co/abhishek/autonlp-japanese-sentiment-59363 をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe62e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/philschmid/serverless-bert-huggingface-aws-lambda-docker/blob/main/get_model.py\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def get_model(model):\n",
    "    \"\"\"Loads model from Hugginface model hub\"\"\"\n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "        model.save_pretrained(OUTPUT_DIR)\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "def get_tokenizer(tokenizer):\n",
    "    \"\"\"Loads tokenizer from Hugginface model hub\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "        tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    except Exception as e:\n",
    "        raise(e)\n",
    "\n",
    "get_model('abhishek/autonlp-japanese-sentiment-59363')\n",
    "get_tokenizer('abhishek/autonlp-japanese-sentiment-59363')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b89f55",
   "metadata": {},
   "source": [
    "以下のセルはLambdaで実行するコードを定義して書き出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a542956",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./lambda-docker/handler.py\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "\n",
    "\n",
    "def encode(tokenizer, inputs):\n",
    "    \"\"\"encodes the question and context with a given tokenizer\"\"\"\n",
    "    encoded = tokenizer.encode_plus(inputs)\n",
    "    return encoded[\"input_ids\"], encoded[\"attention_mask\"]\n",
    "\n",
    "\n",
    "def serverless_pipeline(model_path='./model'):\n",
    "    \"\"\"Initializes the model and tokenzier and returns a predict function that ca be used as pipeline\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    labels = ['LABEL_0', 'LABEL_1']\n",
    "    def predict(inputs):\n",
    "        \"\"\"predicts the answer on an given question and context. Uses encode and decode method from above\"\"\"\n",
    "        input_ids, attention_mask = encode(tokenizer, inputs)\n",
    "        outputs = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))[0]\n",
    "        prob = F.softmax(outputs, dim=1).detach().numpy().astype(np.float64).max()\n",
    "        return {'label': labels[torch.argmax(outputs)], 'score': prob}\n",
    "    return predict\n",
    "\n",
    "\n",
    "# initializes the pipeline\n",
    "sequence_classification_pipeline = serverless_pipeline()\n",
    "\n",
    "\n",
    "def handler(event, context):\n",
    "    try:\n",
    "        print(event)\n",
    "        print(context)\n",
    "        # loads the incoming event into a dictonary\n",
    "        body = json.loads(event['body'])\n",
    "        # uses the pipeline to predict the answer\n",
    "        output = sequence_classification_pipeline(inputs=body['inputs'])\n",
    "        print(output)\n",
    "        return {\n",
    "            \"statusCode\": 200,\n",
    "            \"headers\": {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*',\n",
    "                \"Access-Control-Allow-Credentials\": True\n",
    "            },\n",
    "            \"body\": json.dumps(output)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(repr(e))\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"headers\": {\n",
    "                'Content-Type': 'application/json',\n",
    "                'Access-Control-Allow-Origin': '*',\n",
    "                \"Access-Control-Allow-Credentials\": True\n",
    "            },\n",
    "            \"body\": json.dumps({\"error\": repr(e)})\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28ca6f",
   "metadata": {},
   "source": [
    "## Create an Amazon ECR registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d7e7a",
   "metadata": {},
   "source": [
    "`lambda-docker`ディレクトリ内にある`requirements.txt`と`dockerfile`と`handler.py`、Model、TokenizerをECRへアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43701836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "sess = boto3.Session()\n",
    "\n",
    "registry_name = 'huggingface-lambda-classification'\n",
    "account = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = sess.region_name\n",
    "\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_label = 'v1'\n",
    "image = f'{account}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'\n",
    "\n",
    "%cd lambda-docker\n",
    "!docker build -t {registry_name}:{image_label} .\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66d2e53",
   "metadata": {},
   "source": [
    "## Create a Lambda Function\n",
    "\n",
    "[IAM コンソールでの実行ロールの作成](https://docs.aws.amazon.com/ja_jp/lambda/latest/dg/lambda-intro-execution-role.html)を参照して、\n",
    "IAM コンソールで実行ロールを作成してください。\n",
    "\n",
    "1. IAM コンソールの [Roles (ロール)] ページを開きます。\n",
    "2. [ロールの作成] を選択します。\n",
    "3. [一般的なユースケース] で、[Lambda] を選択します。\n",
    "4. [Next: Permissions (次へ: アクセス許可)] を選択します。\n",
    "5. [アクセス許可ポリシーのアタッチ] で、[AWSLambdaBasicExecutionRole] および [AWSXRayDaemonWriteAccess] AWS 管理ポリシーを選択します。\n",
    "6. [次へ: タグ] を選択します。\n",
    "7. [Next: Review] を選択します。\n",
    "8. [ロール名] に「lambda-role」と入力します（ロール名は任意の名前で構いません）。\n",
    "9. [ロールの作成] を選択します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d600cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = 'YOUR-LAMBDA-ROLE-ARN'\n",
    "fname = 'bert-lambda-classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722bc990",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdac = boto3.client('lambda')\n",
    "\n",
    "dic = {\n",
    "    'FunctionName': fname,\n",
    "    'Role': role,\n",
    "    'Code':{'ImageUri': image},\n",
    "    'Timeout': 60,\n",
    "    'MemorySize': 5120,\n",
    "    'PackageType':'Image'   #add this parameter\n",
    "}\n",
    "\n",
    "lambdac.create_function(**dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011ccfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# サンプルテキストで試しに推論してみましょう\n",
    "payload = {\n",
    "  \"body\": '{\"inputs\": \"ハワイアンの心和む音楽の中、ちょっとシリアスなドラマが展開していきます。音楽の力ってすごいな、って思いました。\"}'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4421fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda関数の作成が完了していない場合、エラーが発生することがあります\n",
    "# 初回の実行には少し時間がかかります\n",
    "import json\n",
    "\n",
    "output = lambdac.invoke(\n",
    "    FunctionName=fname,\n",
    "    Payload= json.dumps(payload),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce17347",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(output['Payload'].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fccc51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
