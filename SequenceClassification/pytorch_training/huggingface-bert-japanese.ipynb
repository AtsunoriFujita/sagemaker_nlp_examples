{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface SageMaker-SDK - BERT Japanese example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    4. [Attach to old training job to an estimator ](#Attach-to-old-training-job-to-an-estimator)  \n",
    "5. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "このnotebookは[HuggingFaceのSageMaker example](https://github.com/huggingface/notebooks/tree/master/sagemaker)内の以下3つのnotebookの内容を統合し、日本語データで動作する様に変更を加えたものです。    \n",
    "- 01_getting_started_pytorch\n",
    "- 05_spot_instances\n",
    "- 06_sagemaker_metrics\n",
    "\n",
    "\n",
    "このデモでは、AmazonSageMakerのHuggingFace Estimatorを使用してSageMakerのトレーニングジョブを実行します。    \n",
    "トレーニングジョブ(`./scripts/train.py`)では`HuggingFace transformers`の`Trainer`クラスを使用してテキストの二値分類を実行します。    \n",
    "トレーニングジョブの実行にはスポットインスタンスを使用します。\n",
    "\n",
    "_**NOTE: このデモは、SagemakerNotebookインスタンス、Sagemaker Studio、ローカル環境で実行できます**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "このNotebookはSageMakerの`conda_pytorch_p36`カーネルを利用しています。    \n",
    "日本語処理のため、`transformers`ではなく`transformers[ja]`をインスールします。\n",
    "\n",
    "_Note: 異なるカーネルを使用する場合はpytorchのインストールも必要になります。_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install \"sagemaker>=2.48.1\" \"transformers[ja]==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ローカル環境でSagemakerを使用する場合はSagemakerに必要な権限を持つIAMロールにアクセスする必要があります。[こちら](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)を参照してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "\n",
    "Amazon の商品レビューデータセットは[Registry of Open Data on AWS](https://registry.opendata.aws/)で公開されており、 以下からダウンロード可能です。    \n",
    "このノートブックでは、日本語のデータセットをダウンロードします。\n",
    "\n",
    "- データセットの概要    \n",
    "https://registry.opendata.aws/amazon-reviews/\n",
    "\n",
    "- 日本語のデータセット(readme.htmlからたどることができます）    \n",
    "https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\n",
    "\n",
    "以下では、データをダウンロードして解凍 (unzip) します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "\n",
    "download_url = \"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_multilingual_JP_v1_00.tsv.gz\" \n",
    "dir_name = \"data\"\n",
    "file_name = \"amazon_review.tsv.gz\"\n",
    "tsv_file_name = \"amazon_review.tsv\"\n",
    "file_path = os.path.join(dir_name,file_name)\n",
    "tsv_file_path = os.path.join(dir_name,tsv_file_name)\n",
    "\n",
    "os.makedirs(dir_name, exist_ok=True)\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    print(\"File {} already exists. Skipped download.\".format(file_name))\n",
    "else:\n",
    "    urllib.request.urlretrieve(download_url, file_path)\n",
    "    print(\"File downloaded: {}\".format(file_path))\n",
    "    \n",
    "if os.path.exists(tsv_file_path):\n",
    "    print(\"File {} already exists. Skipped unzip.\".format(tsv_file_name))\n",
    "else:\n",
    "    with gzip.open(file_path, mode='rb') as fin:\n",
    "        with open(tsv_file_path, 'wb') as fout:\n",
    "            shutil.copyfileobj(fin, fout)\n",
    "            print(\"File uznipped: {}\".format(tsv_file_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "ダウンロードしたデータには学習に不要なデータや直接利用できないデータもあります。以下の前処理で利用できるようにします。\n",
    "\n",
    "1. ダウンロードしたデータには不要なデータも含まれているので削除し、2クラス分類 (positive が 1, negative が 0)となるようにデータを加工します。\n",
    "2. 学習データ、テストデータに分けます。\n",
    "\n",
    "今回利用しないデータは以下の2つです。必要なデータだけ選んで保存します。\n",
    "\n",
    "- star_rating (評価)とreview_body (レビューのテキストデータ)以外のデータ\n",
    "- star_rating が 3 のデータ (positive でも negative でもないデータ)\n",
    "\n",
    "また、評価が1, 2 のデータはラベル 0 (negative) に、評価が4, 5 のデータはラベル 1 (positive) にします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(tsv_file_path, sep ='\\t')\n",
    "df_pos_neg = df.loc[:, [\"star_rating\", \"review_body\"]]\n",
    "df_pos_neg = df_pos_neg[df_pos_neg.star_rating != 3]\n",
    "df_pos_neg.loc[df_pos_neg.star_rating < 3, \"star_rating\"] = 0\n",
    "df_pos_neg.loc[df_pos_neg.star_rating > 3, \"star_rating\"] = 1\n",
    "\n",
    "print(df_pos_neg.shape)\n",
    "df_pos_neg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デモ用にサンプリングしてデータを小さくします\n",
    "df_pos_neg = df_pos_neg.sample(20000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_pos_neg, test_size=0.2, random_state=42)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後の加工のため、データをDataFrameからHuggingface Datasetsクラスに変換します\n",
    "\n",
    "import datasets\n",
    "\n",
    "train = datasets.Dataset.from_pandas(train)\n",
    "test = datasets.Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization \n",
    "\n",
    "`review_body`は文章テキストのままだとモデルの学習ができないため、テキストを意味のある単位で分割（トークナイズ）した上で機械学習モデルで扱える様に数値に変換します。    \n",
    "HuggingFaceのTransformersでは、東北大学が公開している[日本語BERT](https://github.com/cl-tohoku/bert-japanese)の学習済みモデル、トークナイザが使用できるため、このデモではこちらを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BertJapaneseTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "# download tokenizer\n",
    "#tokenizer = BertJapaneseTokenizer.from_pretrained(tokenizer_name)\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['review_body'], max_length=256, padding='max_length', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "試しに一つのテキストでトークナイズと数値変換（エンコード）してみます。    \n",
    "このデモで使用している`cl-tohoku/bert-base-japanese-whole-word-masking`は32,000個の単語と数値のマッピングがあり、[CLS] = 2, 'ハワイ' = 6166, '##アン' = 1028..., [SEP] = 3などとテキストを数値に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original: ', train[0]['review_body'])\n",
    "\n",
    "print('\\nTokenize: ', tokenizer.tokenize(train[0]['review_body']))\n",
    "\n",
    "print('\\nEncode: ', tokenizer.encode(train[0]['review_body']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この処理をデータ全体に適用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize dataset\n",
    "train_dataset = train.map(tokenize, batched=True, batch_size=len(train))\n",
    "test_dataset = test.map(tokenize, batched=True, batch_size=len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_dataset, test_datasetをトレーニングに使用できる様に少し修正します。\n",
    "- `star_rating`を`labels`に変更します（`Trainer`クラスのデフォルトでは`labels`をターゲットカラム名として認識します）。\n",
    "- 学習に使用するカラムをpytorchのフォーマットにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format for pytorch\n",
    "train_dataset.rename_column_(\"star_rating\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n",
    "\n",
    "test_dataset.rename_column_(\"star_rating\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "`datasets`の`FileSystem` [Integration](https://huggingface.co/docs/datasets/filesystems.html)を使用してS3へデータをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "\n",
    "s3_prefix = 'samples/datasets/amazon-review'\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "\n",
    "# 以下のpathにdatasetがuploadされました\n",
    "print(training_input_path)\n",
    "print(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "`HuggingFace`のトレーニングジョブを作成するためには`HuggingFace` Estimatorが必要になります。    \n",
    "Estimatorは、エンドツーエンドのAmazonSageMakerトレーニングおよびデプロイタスクを処理します。 Estimatorで、どのFine-tuningスクリプトを`entry_point`として使用するか、どの`instance_type`を使用するか、どの`hyperparameters`を渡すかなどを定義します。\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    base_job_name='huggingface-sdk-extension',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    role=role,\n",
    "    hyperparameters={\n",
    "        'epochs': 1,\n",
    "        'train_batch_size': 32,\n",
    "        'model_name':'distilbert-base-uncased'\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "SageMakerトレーニングジョブを作成すると、SageMakerは`huggingface`コンテナを実行するために必要なec2インスタンスの起動と管理を行います。    \n",
    "Fine-tuningスクリプト`train.py`をアップロードし、`sagemaker_session_bucket`からコンテナ内の`/opt/ml/input/data`にデータをダウンロードして、トレーニングジョブを実行します。\n",
    "\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "`HuggingFace estimator`で定義した`hyperparameters`は、名前付き引数として渡されます。\n",
    "\n",
    "またSagemakerは、次のようなさまざまな環境変数を通じて、トレーニング環境に関する有用なプロパティを提供しています。\n",
    "\n",
    "* `SM_MODEL_DIR`：トレーニングジョブがモデルアーティファクトを書き込むパスを表す文字列。トレーニング後、このディレクトリのアーティファクトはモデルホスティングのためにS3にアップロードされます。\n",
    "\n",
    "* `SM_NUM_GPUS`：ホストで使用可能なGPUの数を表す整数。\n",
    "\n",
    "* `SM_CHANNEL_XXXX`：指定されたチャネルの入力データを含むディレクトリへのパスを表す文字列。たとえば、HuggingFace estimatorのfit呼び出しで`train`と`test`という名前の2つの入力チャネルを指定すると、環境変数`SM_CHANNEL_TRAIN`と`SM_CHANNEL_TEST`が設定されます。\n",
    "\n",
    "このトレーニングジョブをローカル環境で実行するには、`instance_type='local'`、GPUの場合は`instance_type='local_gpu'`で定義できます。\n",
    "_Note：これはSageMaker Studio内では機能しません_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'epochs': 3,\n",
    "    'train_batch_size': 32,\n",
    "    'learning_rate' : 5e-5,\n",
    "    'model_name':'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    'output_dir':'/opt/ml/checkpoints',\n",
    "    'num_labels': num_labels,\n",
    "}\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = \"using-spot\"\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トレーニングジョブのメトリック抽出に使用する正規表現ベースの定義を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric definition to extract the results\n",
    "metric_definitions=[\n",
    "    {'Name': 'loss', 'Regex': \"'loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'learning_rate', 'Regex': \"'learning_rate': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_accuracy', 'Regex': \"'eval_accuracy': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_f1', 'Regex': \"'eval_f1': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_precision', 'Regex': \"'eval_precision': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_recall', 'Regex': \"'eval_recall': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "    {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name=job_name,\n",
    "    #checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    #use_spot_instances=True,\n",
    "    #max_wait=7200, # This should be equal to or greater than max_run in seconds'\n",
    "    #max_run=3600, # expected max run in seconds\n",
    "    role=role,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n",
    "\n",
    "# データ数: 20,000, 3epochでの実行時間の目安\n",
    "# Training seconds: 1289\n",
    "# Billable seconds: 1289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Training Metrics\n",
    "\n",
    "トレーニングジョブは、メトリックをすぐには出力しません。 たとえば、最初にトレーニングインスタンスをプロビジョニングし、トレーニングイメージをダウンロードし、データをダウンロードする必要があります。 さらに、このデモでは、最初の評価ログは500ステップ後に取得されます（Hugging Face　Trainerクラスのlogging_stepsの[デフォルト](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)。\n",
    "\n",
    "したがって、**トレーニングを開始してから15〜20分後に以下のセクションを実行してください。そうしないと、まだ利用可能な指標がなく、エラーが返される可能性があります**\n",
    "\n",
    "`TrainingJobAnalytics` API callで正確なトレーニングジョブ名を指定することで、このコードをコピーして別の場所から実行することもできます（クラウドに接続され、APIの使用が許可されている場合）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "# Captured metrics can be accessed as a Pandas dataframe\n",
    "df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = df[df.metric_name.isin(['eval_accuracy','eval_precision'])]\n",
    "losses = df[df.metric_name.isin(['loss', 'eval_loss'])]\n",
    "\n",
    "sns.lineplot(\n",
    "    x='timestamp', \n",
    "    y='value', \n",
    "    data=evals, \n",
    "    hue='metric_name', \n",
    "    palette=['blue', 'purple'])\n",
    "\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(\n",
    "    x='timestamp', \n",
    "    y='value', \n",
    "    data=losses, \n",
    "    hue='metric_name', \n",
    "    palette=['orange', 'red'],\n",
    "    ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download-fine-tuned-model-from-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# 学習したモデルのダウンロード\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path='.', # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to old training job to an estimator \n",
    "\n",
    "Sagemakerでは、古いトレーニングジョブをEstimatorにアタッチして、トレーニングを続けたり、結果を取得したりできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
