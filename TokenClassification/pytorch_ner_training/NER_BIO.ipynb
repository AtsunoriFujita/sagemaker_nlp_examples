{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed7387a",
   "metadata": {},
   "source": [
    "# Huggingface SageMaker-SDK - BERT Japanese NER example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e4eef9",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Permissions](#Permissions)\n",
    "    3. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "3. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  \n",
    "    2. [Estimator Parameters](#Estimator-Parameters)   \n",
    "    3. [Download fine-tuned model from s3](#Download-fine-tuned-model-from-s3)\n",
    "    4. [Named Entity Recognition on Local](#Named-Entity-Recognition-on-Local)  \n",
    "4. [_Coming soon_:Push model to the Hugging Face hub](#Push-model-to-the-Hugging-Face-hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a90666",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "このnotebookは書籍：[BERTによる自然言語処理入門 Transformersを使った実践プログラミング](https://www.ohmsha.co.jp/book/9784274227264/)の[固有表現抽出(Named Entity Recognition)](https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb)をAmazon SageMakerで動作する様に変更を加えたものです。    \n",
    "データは[ストックマーク株式会社](https://stockmark.co.jp/)さんで作成された[Wikipediaを用いた日本語の固有表現抽出データセット](https://github.com/stockmarkteam/ner-wikipedia-dataset)を使用します。    \n",
    "\n",
    "このデモでは、AmazonSageMakerのHuggingFace Estimatorを使用してSageMakerのトレーニングジョブを実行します。    \n",
    "\n",
    "_**NOTE: このデモは、SagemakerNotebookインスタンスで動作検証しています**_    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c952a3",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aea1760",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "このNotebookはSageMakerの`conda_pytorch_p36`カーネルを利用しています。    \n",
    "日本語処理のため、`transformers`ではなく`transformers[ja]`をインスールします。\n",
    "\n",
    "**_Note: このnotebook上で推論テストを行う場合、（バージョンが古い場合は）pytorchのバージョンアップが必要になります。_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520a8db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# localで推論のテストを行う場合(CPU)\n",
    "!pip install torch==1.7.1\n",
    "# localで推論のテストを行う場合(GPU)\n",
    "#!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1164f85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.31.0\" \"transformers[ja]==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03a34a",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7224d441",
   "metadata": {},
   "source": [
    "ローカル環境でSagemakerを使用する場合はSagemakerに必要な権限を持つIAMロールにアクセスする必要があります。[こちら](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html)を参照してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc3a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1fd02",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "\n",
    "[ストックマーク株式会社](https://stockmark.co.jp/)さんで作成された[Wikipediaを用いた日本語の固有表現抽出データセット](https://github.com/stockmarkteam/ner-wikipedia-dataset)をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4988ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --branch v2.0 https://github.com/stockmarkteam/ner-wikipedia-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d47bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# データのロード\n",
    "dataset = json.load(open('ner-wikipedia-dataset/ner.json','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f916e49f",
   "metadata": {},
   "source": [
    "データの形式は以下のようになっています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fa59d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248be31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n",
    "import unicodedata\n",
    "\n",
    "# 固有表現のタイプとIDを対応付る辞書 \n",
    "type_id_dict = {\n",
    "    \"人名\": 1,\n",
    "    \"法人名\": 2,\n",
    "    \"政治的組織名\": 3,\n",
    "    \"その他の組織名\": 4,\n",
    "    \"地名\": 5,\n",
    "    \"施設名\": 6,\n",
    "    \"製品名\": 7,\n",
    "    \"イベント名\": 8\n",
    "}\n",
    "\n",
    "# カテゴリーをラベルに変更、文字列の正規化する。\n",
    "for sample in dataset:\n",
    "    sample['text'] = unicodedata.normalize('NFKC', sample['text'])\n",
    "    for e in sample[\"entities\"]:\n",
    "        e['type_id'] = type_id_dict[e['type']]\n",
    "        del e['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b22c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c85fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# データセットの分割\n",
    "random.seed(42)\n",
    "random.shuffle(dataset)\n",
    "n = len(dataset)\n",
    "n_train = int(n*0.6)\n",
    "n_val = int(n*0.2)\n",
    "dataset_train = dataset[:n_train]\n",
    "dataset_val = dataset[n_train:n_train+n_val]\n",
    "dataset_test = dataset[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4bb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n",
    "\n",
    "def create_dataset(tokenizer, dataset, max_length):\n",
    "    \"\"\"\n",
    "    データセットをデータローダに入力できる形に整形。\n",
    "    \"\"\"\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    labels= []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        text = sample['text']\n",
    "        entities = sample['entities']\n",
    "        encoding = tokenizer.encode_plus_tagged(\n",
    "            text, entities, max_length=max_length\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'])\n",
    "        token_type_ids.append(encoding['token_type_ids'])\n",
    "        attention_mask.append(encoding['attention_mask'])\n",
    "        labels.append(encoding['labels'])\n",
    "    \n",
    "    d = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"attention_mask\": attention_mask, \n",
    "        \"labels\": labels\n",
    "    }\n",
    "        \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n",
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "\n",
    "class NER_tokenizer_BIO(BertJapaneseTokenizer):\n",
    "\n",
    "    # 初期化時に固有表現のカテゴリーの数`num_entity_type`を\n",
    "    # 受け入れるようにする。\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.num_entity_type = kwargs.pop('num_entity_type')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def encode_plus_tagged(self, text, entities, max_length):\n",
    "        \"\"\"\n",
    "        文章とそれに含まれる固有表現が与えられた時に、\n",
    "        符号化とラベル列の作成を行う。\n",
    "        \"\"\"\n",
    "        # 固有表現の前後でtextを分割し、それぞれのラベルをつけておく。\n",
    "        splitted = [] # 分割後の文字列を追加していく\n",
    "        position = 0\n",
    "        for entity in entities:\n",
    "            start = entity['span'][0]\n",
    "            end = entity['span'][1]\n",
    "            label = entity['type_id']\n",
    "            splitted.append({'text':text[position:start], 'label':0})\n",
    "            splitted.append({'text':text[start:end], 'label':label})\n",
    "            position = end\n",
    "        splitted.append({'text': text[position:], 'label':0})\n",
    "        splitted = [ s for s in splitted if s['text'] ]\n",
    "\n",
    "        # 分割されたそれぞれの文章をトークン化し、ラベルをつける。\n",
    "        tokens = [] # トークンを追加していく\n",
    "        labels = [] # ラベルを追加していく\n",
    "        for s in splitted:\n",
    "            tokens_splitted = tokenizer.tokenize(s['text'])\n",
    "            label = s['label']\n",
    "            if label > 0: # 固有表現\n",
    "                # まずトークン全てにI-タグを付与\n",
    "                labels_splitted =  \\\n",
    "                    [ label + self.num_entity_type ] * len(tokens_splitted)\n",
    "                # 先頭のトークンをB-タグにする\n",
    "                labels_splitted[0] = label\n",
    "            else: # それ以外\n",
    "                labels_splitted =  [0] * len(tokens_splitted)\n",
    "            \n",
    "            tokens.extend(tokens_splitted)\n",
    "            labels.extend(labels_splitted)\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        encoding = tokenizer.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        ) \n",
    "\n",
    "        # ラベルに特殊トークンを追加\n",
    "        labels = [0] + labels[:max_length-2] + [0]\n",
    "        labels = labels + [0]*( max_length - len(labels) )\n",
    "        encoding['labels'] = labels\n",
    "\n",
    "        return encoding\n",
    "\n",
    "    def encode_plus_untagged(\n",
    "        self, text, max_length=None, return_tensors=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        文章をトークン化し、それぞれのトークンの文章中の位置も特定しておく。\n",
    "        IO法のトークナイザのencode_plus_untaggedと同じ\n",
    "        \"\"\"\n",
    "        # 文章のトークン化を行い、\n",
    "        # それぞれのトークンと文章中の文字列を対応づける。\n",
    "        tokens = [] # トークンを追加していく。\n",
    "        tokens_original = [] # トークンに対応する文章中の文字列を追加していく。\n",
    "        words = self.word_tokenizer.tokenize(text) # MeCabで単語に分割\n",
    "        for word in words:\n",
    "            # 単語をサブワードに分割\n",
    "            tokens_word = self.subword_tokenizer.tokenize(word) \n",
    "            tokens.extend(tokens_word)\n",
    "            if tokens_word[0] == '[UNK]': # 未知語への対応\n",
    "                tokens_original.append(word)\n",
    "            else:\n",
    "                tokens_original.extend([\n",
    "                    token.replace('##','') for token in tokens_word\n",
    "                ])\n",
    "\n",
    "        # 各トークンの文章中での位置を調べる。（空白の位置を考慮する）\n",
    "        position = 0\n",
    "        spans = [] # トークンの位置を追加していく。\n",
    "        for token in tokens_original:\n",
    "            l = len(token)\n",
    "            while 1:\n",
    "                if token != text[position:position+l]:\n",
    "                    position += 1\n",
    "                else:\n",
    "                    spans.append([position, position+l])\n",
    "                    position += l\n",
    "                    break\n",
    "\n",
    "        # 符号化を行いBERTに入力できる形式にする。\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens) \n",
    "        encoding = tokenizer.prepare_for_model(\n",
    "            input_ids, \n",
    "            max_length=max_length, \n",
    "            padding='max_length' if max_length else False, \n",
    "            truncation=True if max_length else False\n",
    "        )\n",
    "        sequence_length = len(encoding['input_ids'])\n",
    "        # 特殊トークン[CLS]に対するダミーのspanを追加。\n",
    "        spans = [[-1, -1]] + spans[:sequence_length-2] \n",
    "        # 特殊トークン[SEP]、[PAD]に対するダミーのspanを追加。\n",
    "        spans = spans + [[-1, -1]] * ( sequence_length - len(spans) ) \n",
    "\n",
    "        # 必要に応じてtorch.Tensorにする。\n",
    "        if return_tensors == 'pt':\n",
    "            encoding = { k: torch.tensor([v]) for k, v in encoding.items() }\n",
    "\n",
    "        return encoding, spans\n",
    "\n",
    "    @staticmethod\n",
    "    def Viterbi(scores_bert, num_entity_type, penalty=10000):\n",
    "        \"\"\"\n",
    "        Viterbiアルゴリズムで最適解を求める。\n",
    "        \"\"\"\n",
    "        m = 2*num_entity_type + 1\n",
    "        penalty_matrix = np.zeros([m, m])\n",
    "        for i in range(m):\n",
    "            for j in range(1+num_entity_type, m):\n",
    "                if not ( (i == j) or (i+num_entity_type == j) ): \n",
    "                    penalty_matrix[i,j] = penalty\n",
    "        \n",
    "        path = [ [i] for i in range(m) ]\n",
    "        scores_path = scores_bert[0] - penalty_matrix[0,:]\n",
    "        scores_bert = scores_bert[1:]\n",
    "\n",
    "        for scores in scores_bert:\n",
    "            assert len(scores) == 2*num_entity_type + 1\n",
    "            score_matrix = np.array(scores_path).reshape(-1,1) \\\n",
    "                + np.array(scores).reshape(1,-1) \\\n",
    "                - penalty_matrix\n",
    "            scores_path = score_matrix.max(axis=0)\n",
    "            argmax = score_matrix.argmax(axis=0)\n",
    "            path_new = []\n",
    "            for i, idx in enumerate(argmax):\n",
    "                path_new.append( path[idx] + [i] )\n",
    "            path = path_new\n",
    "\n",
    "        labels_optimal = path[np.argmax(scores_path)]\n",
    "        return labels_optimal\n",
    "\n",
    "    def convert_bert_output_to_entities(self, text, scores, spans):\n",
    "        \"\"\"\n",
    "        文章、分類スコア、各トークンの位置から固有表現を得る。\n",
    "        分類スコアはサイズが（系列長、ラベル数）の2次元配列\n",
    "        \"\"\"\n",
    "        assert len(spans) == len(scores)\n",
    "        num_entity_type = self.num_entity_type\n",
    "        \n",
    "        # 特殊トークンに対応する部分を取り除く\n",
    "        scores = [score for score, span in zip(scores, spans) if span[0]!=-1]\n",
    "        spans = [span for span in spans if span[0]!=-1]\n",
    "\n",
    "        # Viterbiアルゴリズムでラベルの予測値を決める。\n",
    "        labels = self.Viterbi(scores, num_entity_type)\n",
    "\n",
    "        # 同じラベルが連続するトークンをまとめて、固有表現を抽出する。\n",
    "        entities = []\n",
    "        for label, group \\\n",
    "            in itertools.groupby(enumerate(labels), key=lambda x: x[1]):\n",
    "            \n",
    "            group = list(group)\n",
    "            start = spans[group[0][0]][0]\n",
    "            end = spans[group[-1][0]][1]\n",
    "\n",
    "            if label != 0: # 固有表現であれば\n",
    "                if 1 <= label <= num_entity_type:\n",
    "                     # ラベルが`B-`ならば、新しいentityを追加\n",
    "                    entity = {\n",
    "                        \"name\": text[start:end],\n",
    "                        \"span\": [start, end],\n",
    "                        \"type_id\": label\n",
    "                    }\n",
    "                    entities.append(entity)\n",
    "                else:\n",
    "                    # ラベルが`I-`ならば、直近のentityを更新\n",
    "                    entity['span'][1] = end \n",
    "                    entity['name'] = text[entity['span'][0]:entity['span'][1]]\n",
    "                \n",
    "        return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ebe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "# トークナイザのロード\n",
    "# 固有表現のカテゴリーの数`num_entity_type`を入力に入れる必要がある。\n",
    "tokenizer = NER_tokenizer_BIO.from_pretrained(tokenizer_name, num_entity_type=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "max_length = 128\n",
    "\n",
    "dataset_train = create_dataset(\n",
    "    tokenizer, \n",
    "    dataset_train, \n",
    "    max_length\n",
    ")\n",
    "\n",
    "dataset_val = create_dataset(\n",
    "    tokenizer, \n",
    "    dataset_val, \n",
    "    max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85641f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset_train = datasets.Dataset.from_dict(dataset_train)\n",
    "dataset_val = datasets.Dataset.from_dict(dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79745ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446641b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set format for pytorch\n",
    "dataset_train.set_format('torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])\n",
    "dataset_val.set_format('torch', columns=['input_ids', 'attention_mask', 'token_type_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb88aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f34cb",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "S3へデータをアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e632e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "\n",
    "s3_prefix = 'samples/datasets/ner-wikipedia-dataset-bio'\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "dataset_train.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "dataset_val.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19426bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のpathにdatasetがuploadされました\n",
    "print(training_input_path)\n",
    "print(test_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f795d",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "`HuggingFace`のトレーニングジョブを作成するためには`HuggingFace` Estimatorが必要になります。    \n",
    "Estimatorは、エンドツーエンドのAmazonSageMakerトレーニングおよびデプロイタスクを処理します。 Estimatorで、どのFine-tuningスクリプトを`entry_point`として使用するか、どの`instance_type`を使用するか、どの`hyperparameters`を渡すかなどを定義します。\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    base_job_name='huggingface-sdk-extension',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    role=role,\n",
    "    hyperparameters={\n",
    "        'epochs': 1,\n",
    "        'train_batch_size': 32,\n",
    "        'model_name':'distilbert-base-uncased'\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "SageMakerトレーニングジョブを作成すると、SageMakerは`huggingface`コンテナを実行するために必要なec2インスタンスの起動と管理を行います。    \n",
    "Fine-tuningスクリプト`train.py`をアップロードし、`sagemaker_session_bucket`からコンテナ内の`/opt/ml/input/data`にデータをダウンロードして、トレーニングジョブを実行します。\n",
    "\n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "`HuggingFace estimator`で定義した`hyperparameters`は、名前付き引数として渡されます。\n",
    "\n",
    "またSagemakerは、次のようなさまざまな環境変数を通じて、トレーニング環境に関する有用なプロパティを提供しています。\n",
    "\n",
    "* `SM_MODEL_DIR`：トレーニングジョブがモデルアーティファクトを書き込むパスを表す文字列。トレーニング後、このディレクトリのアーティファクトはモデルホスティングのためにS3にアップロードされます。\n",
    "\n",
    "* `SM_NUM_GPUS`：ホストで使用可能なGPUの数を表す整数。\n",
    "\n",
    "* `SM_CHANNEL_XXXX`：指定されたチャネルの入力データを含むディレクトリへのパスを表す文字列。たとえば、HuggingFace estimatorのfit呼び出しで`train`と`test`という名前の2つの入力チャネルを指定すると、環境変数`SM_CHANNEL_TRAIN`と`SM_CHANNEL_TEST`が設定されます。\n",
    "\n",
    "このトレーニングジョブをローカル環境で実行するには、`instance_type='local'`、GPUの場合は`instance_type='local_gpu'`で定義できます。    \n",
    "**_Note：これはSageMaker Studio内では機能しません_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716cc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングジョブで実行されるコード\n",
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1f65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "num_entity_type = 8\n",
    "num_labels = 2*num_entity_type+1\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'epochs': 5,\n",
    "    'train_batch_size': 32,\n",
    "    'eval_batch_size': 256,\n",
    "    'learning_rate' : 1e-5,\n",
    "    'model_name':'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
    "    'output_dir':'/opt/ml/checkpoints',\n",
    "    'num_labels': num_labels,\n",
    "}\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = \"bert-ner-bio\"\n",
    "#checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450adfa",
   "metadata": {},
   "source": [
    "# Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170844a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name=job_name,\n",
    "    #checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    #use_spot_instances=True,\n",
    "    #max_wait=7200, # This should be equal to or greater than max_run in seconds'\n",
    "    #max_run=3600, # expected max run in seconds\n",
    "    role=role,\n",
    "    transformers_version='4.6',\n",
    "    pytorch_version='1.7',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36490fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})\n",
    "\n",
    "# ml.p3.2xlarge, 5 epochでの実行時間の目安\n",
    "#Training seconds: 558\n",
    "#Billable seconds: 558"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395b68e",
   "metadata": {},
   "source": [
    "# Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321dbe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08699fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c556d4",
   "metadata": {},
   "source": [
    "# Download-fine-tuned-model-from-s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49021353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = './output/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281b703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# 学習したモデルのダウンロード\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path='.', # local path where *.targ.gz is saved\n",
    "    sagemaker_session=sess # sagemaker session used for training the model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a257779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT_DIRに解凍します\n",
    "\n",
    "!tar -zxvf model.tar.gz -C output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c8bac",
   "metadata": {},
   "source": [
    "## Named Entity Recognition on Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dc020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "tokenizer_name = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = NER_tokenizer_BIO.from_pretrained(tokenizer_name, num_entity_type=8)\n",
    "model = AutoModelForTokenClassification.from_pretrained('./output')\n",
    "# model = model.cuda() # GPUで推論する場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fea7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "original_text=[]\n",
    "entities_list = [] # 正解の固有表現を追加していく\n",
    "entities_predicted_list = [] # 抽出された固有表現を追加していく\n",
    "\n",
    "for sample in tqdm(dataset_test):\n",
    "    text = sample['text']\n",
    "    original_text.append(text)\n",
    "    encoding, spans = tokenizer.encode_plus_untagged(\n",
    "        text, return_tensors='pt'\n",
    "    )\n",
    "    #encoding = { k: v.cuda() for k, v in encoding.items() }  # GPUで推論する場合\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "        scores = output.logits\n",
    "        scores = scores[0].cpu().numpy().tolist()\n",
    "        \n",
    "    # 分類スコアを固有表現に変換する\n",
    "    entities_predicted = tokenizer.convert_bert_output_to_entities(\n",
    "        text, scores, spans\n",
    "    )\n",
    "\n",
    "    entities_list.append(sample['entities'])\n",
    "    entities_predicted_list.append( entities_predicted )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd111c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"テキスト: \", original_text[0])\n",
    "print(\"正解: \", entities_list[0])\n",
    "print(\"抽出: \", entities_predicted_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e73f089",
   "metadata": {},
   "source": [
    "# Evaluate NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/stockmarkteam/bert-book/blob/master/Chapter8.ipynb\n",
    "\n",
    "def evaluate_model(entities_list, entities_predicted_list, type_id=None):\n",
    "    \"\"\"\n",
    "    正解と予測を比較し、モデルの固有表現抽出の性能を評価する。\n",
    "    type_idがNoneのときは、全ての固有表現のタイプに対して評価する。\n",
    "    type_idが整数を指定すると、その固有表現のタイプのIDに対して評価を行う。\n",
    "    \"\"\"\n",
    "    num_entities = 0 # 固有表現(正解)の個数\n",
    "    num_predictions = 0 # BERTにより予測された固有表現の個数\n",
    "    num_correct = 0 # BERTにより予測のうち正解であった固有表現の数\n",
    "\n",
    "    # それぞれの文章で予測と正解を比較。\n",
    "    # 予測は文章中の位置とタイプIDが一致すれば正解とみなす。\n",
    "    for entities, entities_predicted in zip(entities_list, entities_predicted_list):\n",
    "\n",
    "        if type_id:\n",
    "            entities = [ e for e in entities if e['type_id'] == type_id ]\n",
    "            entities_predicted = [ \n",
    "                e for e in entities_predicted if e['type_id'] == type_id\n",
    "            ]\n",
    "            \n",
    "        get_span_type = lambda e: (e['span'][0], e['span'][1], e['type_id'])\n",
    "        set_entities = set(get_span_type(e) for e in entities)\n",
    "        set_entities_predicted = set(get_span_type(e) for e in entities_predicted)\n",
    "\n",
    "        num_entities += len(entities)\n",
    "        num_predictions += len(entities_predicted)\n",
    "        num_correct += len( set_entities & set_entities_predicted )\n",
    "\n",
    "    # 指標を計算\n",
    "    precision = num_correct/num_predictions # 適合率\n",
    "    recall = num_correct/num_entities # 再現率\n",
    "    f_value = 2*precision*recall/(precision+recall) # F値\n",
    "\n",
    "    result = {\n",
    "        'num_entities': num_entities,\n",
    "        'num_predictions': num_predictions,\n",
    "        'num_correct': num_correct,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f_value': f_value\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_model(entities_list, entities_predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff22d609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
